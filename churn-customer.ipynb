{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a50c980",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:13.605904Z",
     "iopub.status.busy": "2022-11-02T13:14:13.604900Z",
     "iopub.status.idle": "2022-11-02T13:14:13.626120Z",
     "shell.execute_reply": "2022-11-02T13:14:13.624972Z"
    },
    "papermill": {
     "duration": 0.042559,
     "end_time": "2022-11-02T13:14:13.628929",
     "exception": false,
     "start_time": "2022-11-02T13:14:13.586370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/brazilian-ecommerce/olist_customers_dataset.csv\n",
      "/kaggle/input/brazilian-ecommerce/olist_sellers_dataset.csv\n",
      "/kaggle/input/brazilian-ecommerce/olist_order_reviews_dataset.csv\n",
      "/kaggle/input/brazilian-ecommerce/olist_order_items_dataset.csv\n",
      "/kaggle/input/brazilian-ecommerce/olist_products_dataset.csv\n",
      "/kaggle/input/brazilian-ecommerce/olist_geolocation_dataset.csv\n",
      "/kaggle/input/brazilian-ecommerce/product_category_name_translation.csv\n",
      "/kaggle/input/brazilian-ecommerce/olist_orders_dataset.csv\n",
      "/kaggle/input/brazilian-ecommerce/olist_order_payments_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acc946",
   "metadata": {
    "papermill": {
     "duration": 0.015741,
     "end_time": "2022-11-02T13:14:13.659193",
     "exception": false,
     "start_time": "2022-11-02T13:14:13.643452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Link Kaggle : https://www.kaggle.com/jayzainab/tugas-5-sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474560fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:13.690336Z",
     "iopub.status.busy": "2022-11-02T13:14:13.689797Z",
     "iopub.status.idle": "2022-11-02T13:14:14.807164Z",
     "shell.execute_reply": "2022-11-02T13:14:14.806075Z"
    },
    "papermill": {
     "duration": 1.13605,
     "end_time": "2022-11-02T13:14:14.809839",
     "exception": false,
     "start_time": "2022-11-02T13:14:13.673789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea3c5e",
   "metadata": {
    "papermill": {
     "duration": 0.013522,
     "end_time": "2022-11-02T13:14:14.837557",
     "exception": false,
     "start_time": "2022-11-02T13:14:14.824035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48a844f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:14.867269Z",
     "iopub.status.busy": "2022-11-02T13:14:14.866865Z",
     "iopub.status.idle": "2022-11-02T13:14:16.762218Z",
     "shell.execute_reply": "2022-11-02T13:14:16.761141Z"
    },
    "papermill": {
     "duration": 1.913626,
     "end_time": "2022-11-02T13:14:16.765054",
     "exception": false,
     "start_time": "2022-11-02T13:14:14.851428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_customers = pd.read_csv('../input/brazilian-ecommerce/olist_customers_dataset.csv')\n",
    "df_order_items = pd.read_csv('../input/brazilian-ecommerce/olist_order_items_dataset.csv')\n",
    "df_order_payments = pd.read_csv('../input/brazilian-ecommerce/olist_order_payments_dataset.csv')\n",
    "df_orders = pd.read_csv('../input/brazilian-ecommerce/olist_orders_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a4bc8",
   "metadata": {
    "papermill": {
     "duration": 0.013984,
     "end_time": "2022-11-02T13:14:16.793577",
     "exception": false,
     "start_time": "2022-11-02T13:14:16.779593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64166fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:16.823389Z",
     "iopub.status.busy": "2022-11-02T13:14:16.822999Z",
     "iopub.status.idle": "2022-11-02T13:14:17.316977Z",
     "shell.execute_reply": "2022-11-02T13:14:17.315873Z"
    },
    "papermill": {
     "duration": 0.51171,
     "end_time": "2022-11-02T13:14:17.319264",
     "exception": false,
     "start_time": "2022-11-02T13:14:16.807554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_sequential</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>1</td>\n",
       "      <td>18.12</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>voucher</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>voucher</td>\n",
       "      <td>1</td>\n",
       "      <td>18.59</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13 00:00:00</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>boleto</td>\n",
       "      <td>1</td>\n",
       "      <td>141.46</td>\n",
       "      <td>1</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>289cdb325fb7e7f891c38608bf9e0962</td>\n",
       "      <td>2018-07-30 03:24:27</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04 00:00:00</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>3</td>\n",
       "      <td>179.12</td>\n",
       "      <td>1</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>4869f7a5dfa277a7dca6462dcf3b52b2</td>\n",
       "      <td>2018-08-13 08:55:23</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp    order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date                customer_unique_id  \\\n",
       "0           2017-10-18 00:00:00  7c396fd4830fd04220f754e42b4e5bff   \n",
       "1           2017-10-18 00:00:00  7c396fd4830fd04220f754e42b4e5bff   \n",
       "2           2017-10-18 00:00:00  7c396fd4830fd04220f754e42b4e5bff   \n",
       "3           2018-08-13 00:00:00  af07308b275d755c9edb36a90c618231   \n",
       "4           2018-09-04 00:00:00  3a653a41f6f9fc3d2a113cf8398680e8   \n",
       "\n",
       "   customer_zip_code_prefix  ... payment_sequential payment_type  \\\n",
       "0                      3149  ...                  1  credit_card   \n",
       "1                      3149  ...                  3      voucher   \n",
       "2                      3149  ...                  2      voucher   \n",
       "3                     47813  ...                  1       boleto   \n",
       "4                     75265  ...                  1  credit_card   \n",
       "\n",
       "   payment_installments payment_value  order_item_id  \\\n",
       "0                     1         18.12              1   \n",
       "1                     1          2.00              1   \n",
       "2                     1         18.59              1   \n",
       "3                     1        141.46              1   \n",
       "4                     3        179.12              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "1  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "2  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "3  595fac2a385ac33a80bd5114aec74eb8  289cdb325fb7e7f891c38608bf9e0962   \n",
       "4  aa4383b373c6aca5d8797843e5594415  4869f7a5dfa277a7dca6462dcf3b52b2   \n",
       "\n",
       "   shipping_limit_date   price freight_value  \n",
       "0  2017-10-06 11:07:15   29.99          8.72  \n",
       "1  2017-10-06 11:07:15   29.99          8.72  \n",
       "2  2017-10-06 11:07:15   29.99          8.72  \n",
       "3  2018-07-30 03:24:27  118.70         22.76  \n",
       "4  2018-08-13 08:55:23  159.90         19.22  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_orders.merge(df_customers, on = 'customer_id')\n",
    "df_temp = df_temp.merge(df_order_payments, on = 'order_id')\n",
    "df_temp = df_temp.merge(df_order_items, on = 'order_id')\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24987d24",
   "metadata": {
    "papermill": {
     "duration": 0.014124,
     "end_time": "2022-11-02T13:14:17.348024",
     "exception": false,
     "start_time": "2022-11-02T13:14:17.333900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Future Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a4eaae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:17.378601Z",
     "iopub.status.busy": "2022-11-02T13:14:17.378150Z",
     "iopub.status.idle": "2022-11-02T13:14:17.551184Z",
     "shell.execute_reply": "2022-11-02T13:14:17.549912Z"
    },
    "papermill": {
     "duration": 0.191527,
     "end_time": "2022-11-02T13:14:17.553903",
     "exception": false,
     "start_time": "2022-11-02T13:14:17.362376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MENGUBAH KE DATETIME\n",
    "\n",
    "df_temp['order_delivered_customer_date'] = pd.to_datetime(df_temp['order_delivered_customer_date'], format = '%Y-%m-%d %H:%M:%S')\n",
    "df_temp['order_purchase_timestamp'] = pd.to_datetime(df_temp['order_purchase_timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "df_temp['order_approved_at'] = pd.to_datetime(df_temp['order_approved_at'], format = '%Y-%m-%d %H:%M:%S')\n",
    "df_temp['order_delivered_carrier_date'] = pd.to_datetime(df_temp['order_delivered_carrier_date'], format = '%Y-%m-%d %H:%M:%S')\n",
    "df_temp['order_estimated_delivery_date'] = pd.to_datetime(df_temp['order_estimated_delivery_date'], format = '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66764639",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:17.585384Z",
     "iopub.status.busy": "2022-11-02T13:14:17.584970Z",
     "iopub.status.idle": "2022-11-02T13:14:17.765333Z",
     "shell.execute_reply": "2022-11-02T13:14:17.764033Z"
    },
    "papermill": {
     "duration": 0.199019,
     "end_time": "2022-11-02T13:14:17.767846",
     "exception": false,
     "start_time": "2022-11-02T13:14:17.568827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>most_recent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00012a2ce6f8dcda20d059ce98491703</td>\n",
       "      <td>2017-11-14 16:35:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000161a058600d5901f007fab4c27140</td>\n",
       "      <td>2017-07-16 09:55:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001fd6190edaaf884bcaf3d49edf079</td>\n",
       "      <td>2017-02-28 11:15:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002414f95344307404f0ace7a26f1d5</td>\n",
       "      <td>2017-08-17 03:10:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000379cdec625522490c315e70c7a9fb</td>\n",
       "      <td>2018-04-04 03:10:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id         most_recent\n",
       "0  00012a2ce6f8dcda20d059ce98491703 2017-11-14 16:35:32\n",
       "1  000161a058600d5901f007fab4c27140 2017-07-16 09:55:12\n",
       "2  0001fd6190edaaf884bcaf3d49edf079 2017-02-28 11:15:20\n",
       "3  0002414f95344307404f0ace7a26f1d5 2017-08-17 03:10:27\n",
       "4  000379cdec625522490c315e70c7a9fb 2018-04-04 03:10:19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat feature berapa hari sejak pembelian terakhir oleh customer\n",
    "\n",
    "df_recency= df_temp.groupby('customer_id')['order_approved_at'].max().reset_index()\n",
    "df_recency = df_recency.rename({'order_approved_at':'most_recent'},axis = 1)\n",
    "df_recency['most_recent'] = pd.to_datetime(df_recency.most_recent).dt.tz_localize(None)\n",
    "df_recency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b553c54a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:17.798833Z",
     "iopub.status.busy": "2022-11-02T13:14:17.798432Z",
     "iopub.status.idle": "2022-11-02T13:14:17.817644Z",
     "shell.execute_reply": "2022-11-02T13:14:17.816354Z"
    },
    "papermill": {
     "duration": 0.037629,
     "end_time": "2022-11-02T13:14:17.820168",
     "exception": false,
     "start_time": "2022-11-02T13:14:17.782539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>most_recent</th>\n",
       "      <th>recency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00012a2ce6f8dcda20d059ce98491703</td>\n",
       "      <td>2017-11-14 16:35:32</td>\n",
       "      <td>293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000161a058600d5901f007fab4c27140</td>\n",
       "      <td>2017-07-16 09:55:12</td>\n",
       "      <td>414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001fd6190edaaf884bcaf3d49edf079</td>\n",
       "      <td>2017-02-28 11:15:20</td>\n",
       "      <td>552.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002414f95344307404f0ace7a26f1d5</td>\n",
       "      <td>2017-08-17 03:10:27</td>\n",
       "      <td>382.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000379cdec625522490c315e70c7a9fb</td>\n",
       "      <td>2018-04-04 03:10:19</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id         most_recent  recency\n",
       "0  00012a2ce6f8dcda20d059ce98491703 2017-11-14 16:35:32    293.0\n",
       "1  000161a058600d5901f007fab4c27140 2017-07-16 09:55:12    414.0\n",
       "2  0001fd6190edaaf884bcaf3d49edf079 2017-02-28 11:15:20    552.0\n",
       "3  0002414f95344307404f0ace7a26f1d5 2017-08-17 03:10:27    382.0\n",
       "4  000379cdec625522490c315e70c7a9fb 2018-04-04 03:10:19    152.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recency['recency'] = (df_recency['most_recent'].max() - df_recency['most_recent']).dt.days\n",
    "df_recency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1beebcaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:17.853243Z",
     "iopub.status.busy": "2022-11-02T13:14:17.851552Z",
     "iopub.status.idle": "2022-11-02T13:14:18.115358Z",
     "shell.execute_reply": "2022-11-02T13:14:18.114323Z"
    },
    "papermill": {
     "duration": 0.282578,
     "end_time": "2022-11-02T13:14:18.117689",
     "exception": false,
     "start_time": "2022-11-02T13:14:17.835111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting distribusi recency\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.histplot(x='recency', data=df_recency, bins=20)\n",
    "plt.title('Distribusi recency')\n",
    "plt.xlabel('recency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041398c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:18.150496Z",
     "iopub.status.busy": "2022-11-02T13:14:18.149668Z",
     "iopub.status.idle": "2022-11-02T13:14:18.302865Z",
     "shell.execute_reply": "2022-11-02T13:14:18.301712Z"
    },
    "papermill": {
     "duration": 0.172397,
     "end_time": "2022-11-02T13:14:18.305108",
     "exception": false,
     "start_time": "2022-11-02T13:14:18.132711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_installments</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>most_recent</th>\n",
       "      <th>recency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18.12</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18.59</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>141.46</td>\n",
       "      <td>1</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>289cdb325fb7e7f891c38608bf9e0962</td>\n",
       "      <td>2018-07-30 03:24:27</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>179.12</td>\n",
       "      <td>1</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>4869f7a5dfa277a7dca6462dcf3b52b2</td>\n",
       "      <td>2018-08-13 08:55:23</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date                customer_unique_id  \\\n",
       "0                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "1                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "2                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "3                    2018-08-13  af07308b275d755c9edb36a90c618231   \n",
       "4                    2018-09-04  3a653a41f6f9fc3d2a113cf8398680e8   \n",
       "\n",
       "   customer_zip_code_prefix  ... payment_installments payment_value  \\\n",
       "0                      3149  ...                    1         18.12   \n",
       "1                      3149  ...                    1          2.00   \n",
       "2                      3149  ...                    1         18.59   \n",
       "3                     47813  ...                    1        141.46   \n",
       "4                     75265  ...                    3        179.12   \n",
       "\n",
       "   order_item_id                        product_id  \\\n",
       "0              1  87285b34884572647811a353c7ac498a   \n",
       "1              1  87285b34884572647811a353c7ac498a   \n",
       "2              1  87285b34884572647811a353c7ac498a   \n",
       "3              1  595fac2a385ac33a80bd5114aec74eb8   \n",
       "4              1  aa4383b373c6aca5d8797843e5594415   \n",
       "\n",
       "                          seller_id  shipping_limit_date   price  \\\n",
       "0  3504c0cb71d7fa48d967e0e4c94d59d9  2017-10-06 11:07:15   29.99   \n",
       "1  3504c0cb71d7fa48d967e0e4c94d59d9  2017-10-06 11:07:15   29.99   \n",
       "2  3504c0cb71d7fa48d967e0e4c94d59d9  2017-10-06 11:07:15   29.99   \n",
       "3  289cdb325fb7e7f891c38608bf9e0962  2018-07-30 03:24:27  118.70   \n",
       "4  4869f7a5dfa277a7dca6462dcf3b52b2  2018-08-13 08:55:23  159.90   \n",
       "\n",
       "  freight_value         most_recent recency  \n",
       "0          8.72 2017-10-02 11:07:15   336.0  \n",
       "1          8.72 2017-10-02 11:07:15   336.0  \n",
       "2          8.72 2017-10-02 11:07:15   336.0  \n",
       "3         22.76 2018-07-26 03:24:27    39.0  \n",
       "4         19.22 2018-08-08 08:55:23    26.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_temp.merge(df_recency, on = 'customer_id')\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b83a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:18.338073Z",
     "iopub.status.busy": "2022-11-02T13:14:18.337197Z",
     "iopub.status.idle": "2022-11-02T13:14:18.562205Z",
     "shell.execute_reply": "2022-11-02T13:14:18.561059Z"
    },
    "papermill": {
     "duration": 0.244153,
     "end_time": "2022-11-02T13:14:18.564693",
     "exception": false,
     "start_time": "2022-11-02T13:14:18.320540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00012a2ce6f8dcda20d059ce98491703</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000161a058600d5901f007fab4c27140</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001fd6190edaaf884bcaf3d49edf079</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002414f95344307404f0ace7a26f1d5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000379cdec625522490c315e70c7a9fb</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  frequency\n",
       "customer_id                                \n",
       "00012a2ce6f8dcda20d059ce98491703          1\n",
       "000161a058600d5901f007fab4c27140          1\n",
       "0001fd6190edaaf884bcaf3d49edf079          1\n",
       "0002414f95344307404f0ace7a26f1d5          1\n",
       "000379cdec625522490c315e70c7a9fb          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat kolom customer yang melakukan pembelian kembali (Frequency)\n",
    "\n",
    "df_frequency = df_temp[['order_approved_at','customer_id']].groupby('customer_id').count()\n",
    "df_frequency.rename(columns={'order_approved_at':'frequency'}, inplace=True)\n",
    "df_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965e5690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:18.599038Z",
     "iopub.status.busy": "2022-11-02T13:14:18.597901Z",
     "iopub.status.idle": "2022-11-02T13:14:18.916517Z",
     "shell.execute_reply": "2022-11-02T13:14:18.915318Z"
    },
    "papermill": {
     "duration": 0.338222,
     "end_time": "2022-11-02T13:14:18.919107",
     "exception": false,
     "start_time": "2022-11-02T13:14:18.580885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting distribusi frequency\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.histplot(x='frequency', data=df_frequency, bins=20)\n",
    "plt.title('Distribusi Frequency')\n",
    "plt.xlabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c88cb6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:18.954104Z",
     "iopub.status.busy": "2022-11-02T13:14:18.953664Z",
     "iopub.status.idle": "2022-11-02T13:14:19.242634Z",
     "shell.execute_reply": "2022-11-02T13:14:19.241313Z"
    },
    "papermill": {
     "duration": 0.309305,
     "end_time": "2022-11-02T13:14:19.244911",
     "exception": false,
     "start_time": "2022-11-02T13:14:18.935606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_value</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>most_recent</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>18.12</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>18.59</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>...</td>\n",
       "      <td>141.46</td>\n",
       "      <td>1</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>289cdb325fb7e7f891c38608bf9e0962</td>\n",
       "      <td>2018-07-30 03:24:27</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>...</td>\n",
       "      <td>179.12</td>\n",
       "      <td>1</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>4869f7a5dfa277a7dca6462dcf3b52b2</td>\n",
       "      <td>2018-08-13 08:55:23</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date                customer_unique_id  \\\n",
       "0                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "1                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "2                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "3                    2018-08-13  af07308b275d755c9edb36a90c618231   \n",
       "4                    2018-09-04  3a653a41f6f9fc3d2a113cf8398680e8   \n",
       "\n",
       "   customer_zip_code_prefix  ... payment_value order_item_id  \\\n",
       "0                      3149  ...         18.12             1   \n",
       "1                      3149  ...          2.00             1   \n",
       "2                      3149  ...         18.59             1   \n",
       "3                     47813  ...        141.46             1   \n",
       "4                     75265  ...        179.12             1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "1  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "2  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "3  595fac2a385ac33a80bd5114aec74eb8  289cdb325fb7e7f891c38608bf9e0962   \n",
       "4  aa4383b373c6aca5d8797843e5594415  4869f7a5dfa277a7dca6462dcf3b52b2   \n",
       "\n",
       "   shipping_limit_date   price  freight_value         most_recent recency  \\\n",
       "0  2017-10-06 11:07:15   29.99           8.72 2017-10-02 11:07:15   336.0   \n",
       "1  2017-10-06 11:07:15   29.99           8.72 2017-10-02 11:07:15   336.0   \n",
       "2  2017-10-06 11:07:15   29.99           8.72 2017-10-02 11:07:15   336.0   \n",
       "3  2018-07-30 03:24:27  118.70          22.76 2018-07-26 03:24:27    39.0   \n",
       "4  2018-08-13 08:55:23  159.90          19.22 2018-08-08 08:55:23    26.0   \n",
       "\n",
       "  frequency  \n",
       "0         3  \n",
       "1         3  \n",
       "2         3  \n",
       "3         1  \n",
       "4         1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_temp.merge(df_frequency, on = 'customer_id')\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8499e42a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:19.280295Z",
     "iopub.status.busy": "2022-11-02T13:14:19.279024Z",
     "iopub.status.idle": "2022-11-02T13:14:19.439421Z",
     "shell.execute_reply": "2022-11-02T13:14:19.438511Z"
    },
    "papermill": {
     "duration": 0.180198,
     "end_time": "2022-11-02T13:14:19.441571",
     "exception": false,
     "start_time": "2022-11-02T13:14:19.261373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00012a2ce6f8dcda20d059ce98491703</td>\n",
       "      <td>114.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000161a058600d5901f007fab4c27140</td>\n",
       "      <td>67.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001fd6190edaaf884bcaf3d49edf079</td>\n",
       "      <td>195.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002414f95344307404f0ace7a26f1d5</td>\n",
       "      <td>179.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000379cdec625522490c315e70c7a9fb</td>\n",
       "      <td>107.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id  monetary\n",
       "0  00012a2ce6f8dcda20d059ce98491703    114.74\n",
       "1  000161a058600d5901f007fab4c27140     67.41\n",
       "2  0001fd6190edaaf884bcaf3d49edf079    195.42\n",
       "3  0002414f95344307404f0ace7a26f1d5    179.35\n",
       "4  000379cdec625522490c315e70c7a9fb    107.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat kolom pembelian tiap customer (Monetary)\n",
    "\n",
    "df_monetary = df_temp.groupby('customer_id')['payment_value'].sum().reset_index()\n",
    "df_monetary = df_monetary.rename({'payment_value':'monetary'},axis = 1)\n",
    "df_monetary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06fb4943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:19.477387Z",
     "iopub.status.busy": "2022-11-02T13:14:19.476071Z",
     "iopub.status.idle": "2022-11-02T13:14:19.692010Z",
     "shell.execute_reply": "2022-11-02T13:14:19.690659Z"
    },
    "papermill": {
     "duration": 0.236256,
     "end_time": "2022-11-02T13:14:19.694490",
     "exception": false,
     "start_time": "2022-11-02T13:14:19.458234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting distribusi total purchase\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.histplot(x='monetary', data=df_monetary, bins=20)\n",
    "plt.title('Distribusi Total Pembelian')\n",
    "plt.xlabel('Total Pembelian')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203d9bba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:19.729894Z",
     "iopub.status.busy": "2022-11-02T13:14:19.729477Z",
     "iopub.status.idle": "2022-11-02T13:14:19.866045Z",
     "shell.execute_reply": "2022-11-02T13:14:19.864906Z"
    },
    "papermill": {
     "duration": 0.157298,
     "end_time": "2022-11-02T13:14:19.868639",
     "exception": false,
     "start_time": "2022-11-02T13:14:19.711341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>most_recent</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>289cdb325fb7e7f891c38608bf9e0962</td>\n",
       "      <td>2018-07-30 03:24:27</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>141.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>4869f7a5dfa277a7dca6462dcf3b52b2</td>\n",
       "      <td>2018-08-13 08:55:23</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>179.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date                customer_unique_id  \\\n",
       "0                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "1                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "2                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "3                    2018-08-13  af07308b275d755c9edb36a90c618231   \n",
       "4                    2018-09-04  3a653a41f6f9fc3d2a113cf8398680e8   \n",
       "\n",
       "   customer_zip_code_prefix  ... order_item_id  \\\n",
       "0                      3149  ...             1   \n",
       "1                      3149  ...             1   \n",
       "2                      3149  ...             1   \n",
       "3                     47813  ...             1   \n",
       "4                     75265  ...             1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "1  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "2  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "3  595fac2a385ac33a80bd5114aec74eb8  289cdb325fb7e7f891c38608bf9e0962   \n",
       "4  aa4383b373c6aca5d8797843e5594415  4869f7a5dfa277a7dca6462dcf3b52b2   \n",
       "\n",
       "   shipping_limit_date   price  freight_value         most_recent recency  \\\n",
       "0  2017-10-06 11:07:15   29.99           8.72 2017-10-02 11:07:15   336.0   \n",
       "1  2017-10-06 11:07:15   29.99           8.72 2017-10-02 11:07:15   336.0   \n",
       "2  2017-10-06 11:07:15   29.99           8.72 2017-10-02 11:07:15   336.0   \n",
       "3  2018-07-30 03:24:27  118.70          22.76 2018-07-26 03:24:27    39.0   \n",
       "4  2018-08-13 08:55:23  159.90          19.22 2018-08-08 08:55:23    26.0   \n",
       "\n",
       "  frequency monetary  \n",
       "0         3    38.71  \n",
       "1         3    38.71  \n",
       "2         3    38.71  \n",
       "3         1   141.46  \n",
       "4         1   179.12  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_temp.merge(df_monetary, on = 'customer_id')\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba4040",
   "metadata": {
    "papermill": {
     "duration": 0.016991,
     "end_time": "2022-11-02T13:14:19.903519",
     "exception": false,
     "start_time": "2022-11-02T13:14:19.886528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Finding Churn Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e18c02c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:19.940119Z",
     "iopub.status.busy": "2022-11-02T13:14:19.939714Z",
     "iopub.status.idle": "2022-11-02T13:14:20.005852Z",
     "shell.execute_reply": "2022-11-02T13:14:20.004688Z"
    },
    "papermill": {
     "duration": 0.087507,
     "end_time": "2022-11-02T13:14:20.008204",
     "exception": false,
     "start_time": "2022-11-02T13:14:19.920697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>most_recent</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>3149</td>\n",
       "      <td>...</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>47813</td>\n",
       "      <td>...</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>289cdb325fb7e7f891c38608bf9e0962</td>\n",
       "      <td>2018-07-30 03:24:27</td>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>141.46</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>75265</td>\n",
       "      <td>...</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>4869f7a5dfa277a7dca6462dcf3b52b2</td>\n",
       "      <td>2018-08-13 08:55:23</td>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>179.12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "2    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "3    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "4    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date                customer_unique_id  \\\n",
       "0                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "1                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "2                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
       "3                    2018-08-13  af07308b275d755c9edb36a90c618231   \n",
       "4                    2018-09-04  3a653a41f6f9fc3d2a113cf8398680e8   \n",
       "\n",
       "   customer_zip_code_prefix  ...                        product_id  \\\n",
       "0                      3149  ...  87285b34884572647811a353c7ac498a   \n",
       "1                      3149  ...  87285b34884572647811a353c7ac498a   \n",
       "2                      3149  ...  87285b34884572647811a353c7ac498a   \n",
       "3                     47813  ...  595fac2a385ac33a80bd5114aec74eb8   \n",
       "4                     75265  ...  aa4383b373c6aca5d8797843e5594415   \n",
       "\n",
       "                          seller_id  shipping_limit_date   price  \\\n",
       "0  3504c0cb71d7fa48d967e0e4c94d59d9  2017-10-06 11:07:15   29.99   \n",
       "1  3504c0cb71d7fa48d967e0e4c94d59d9  2017-10-06 11:07:15   29.99   \n",
       "2  3504c0cb71d7fa48d967e0e4c94d59d9  2017-10-06 11:07:15   29.99   \n",
       "3  289cdb325fb7e7f891c38608bf9e0962  2018-07-30 03:24:27  118.70   \n",
       "4  4869f7a5dfa277a7dca6462dcf3b52b2  2018-08-13 08:55:23  159.90   \n",
       "\n",
       "   freight_value         most_recent  recency frequency monetary is_churn  \n",
       "0           8.72 2017-10-02 11:07:15    336.0         3    38.71    False  \n",
       "1           8.72 2017-10-02 11:07:15    336.0         3    38.71    False  \n",
       "2           8.72 2017-10-02 11:07:15    336.0         3    38.71    False  \n",
       "3          22.76 2018-07-26 03:24:27     39.0         1   141.46    False  \n",
       "4          19.22 2018-08-08 08:55:23     26.0         1   179.12    False  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat kolom churn : 365 Hari sejak pembelian terakhir tidak order lagi\n",
    "\n",
    "df_temp['is_churn'] = df_temp['recency'].apply(lambda x: True if x >= 365 else False)\n",
    "df_temp['is_churn'].value_counts()\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70c334ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.045017Z",
     "iopub.status.busy": "2022-11-02T13:14:20.044609Z",
     "iopub.status.idle": "2022-11-02T13:14:20.182299Z",
     "shell.execute_reply": "2022-11-02T13:14:20.180408Z"
    },
    "papermill": {
     "duration": 0.161252,
     "end_time": "2022-11-02T13:14:20.186911",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.025659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_churn = df_temp.groupby(['is_churn'])['customer_id'].count()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Proporsi Persentase Jumlah Churn Customer')\n",
    "\n",
    "df_churn.plot.pie(autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732d4c3",
   "metadata": {
    "papermill": {
     "duration": 0.037646,
     "end_time": "2022-11-02T13:14:20.271610",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.233964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4230acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.308617Z",
     "iopub.status.busy": "2022-11-02T13:14:20.308213Z",
     "iopub.status.idle": "2022-11-02T13:14:20.400919Z",
     "shell.execute_reply": "2022-11-02T13:14:20.398939Z"
    },
    "papermill": {
     "duration": 0.114766,
     "end_time": "2022-11-02T13:14:20.403912",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.289146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 117601 entries, 0 to 117600\n",
      "Data columns (total 27 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   order_id                       117601 non-null  object        \n",
      " 1   customer_id                    117601 non-null  object        \n",
      " 2   order_status                   117601 non-null  object        \n",
      " 3   order_purchase_timestamp       117601 non-null  datetime64[ns]\n",
      " 4   order_approved_at              117586 non-null  datetime64[ns]\n",
      " 5   order_delivered_carrier_date   116356 non-null  datetime64[ns]\n",
      " 6   order_delivered_customer_date  115034 non-null  datetime64[ns]\n",
      " 7   order_estimated_delivery_date  117601 non-null  datetime64[ns]\n",
      " 8   customer_unique_id             117601 non-null  object        \n",
      " 9   customer_zip_code_prefix       117601 non-null  int64         \n",
      " 10  customer_city                  117601 non-null  object        \n",
      " 11  customer_state                 117601 non-null  object        \n",
      " 12  payment_sequential             117601 non-null  int64         \n",
      " 13  payment_type                   117601 non-null  object        \n",
      " 14  payment_installments           117601 non-null  int64         \n",
      " 15  payment_value                  117601 non-null  float64       \n",
      " 16  order_item_id                  117601 non-null  int64         \n",
      " 17  product_id                     117601 non-null  object        \n",
      " 18  seller_id                      117601 non-null  object        \n",
      " 19  shipping_limit_date            117601 non-null  object        \n",
      " 20  price                          117601 non-null  float64       \n",
      " 21  freight_value                  117601 non-null  float64       \n",
      " 22  most_recent                    117586 non-null  datetime64[ns]\n",
      " 23  recency                        117586 non-null  float64       \n",
      " 24  frequency                      117601 non-null  int64         \n",
      " 25  monetary                       117601 non-null  float64       \n",
      " 26  is_churn                       117601 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](6), float64(5), int64(5), object(10)\n",
      "memory usage: 24.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "190fefb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.441725Z",
     "iopub.status.busy": "2022-11-02T13:14:20.441323Z",
     "iopub.status.idle": "2022-11-02T13:14:20.449188Z",
     "shell.execute_reply": "2022-11-02T13:14:20.447872Z"
    },
    "papermill": {
     "duration": 0.02954,
     "end_time": "2022-11-02T13:14:20.451550",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.422010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mengisi Missing Value  Recency dengan Mean\n",
    "\n",
    "df_temp.recency = df_temp.recency.fillna(df_temp.recency.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "907ad844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.490525Z",
     "iopub.status.busy": "2022-11-02T13:14:20.490103Z",
     "iopub.status.idle": "2022-11-02T13:14:20.535005Z",
     "shell.execute_reply": "2022-11-02T13:14:20.533462Z"
    },
    "papermill": {
     "duration": 0.067028,
     "end_time": "2022-11-02T13:14:20.537552",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.470524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_temp['order_approved_at'].fillna(df_temp['order_approved_at'].mode()[0], inplace=True)\n",
    "df_temp['order_delivered_carrier_date'].fillna(df_temp['order_delivered_carrier_date'].mode()[0], inplace=True)\n",
    "df_temp['order_delivered_customer_date'].fillna(df_temp['order_delivered_customer_date'].mode()[0], inplace=True)\n",
    "df_temp['most_recent'].fillna(df_temp['most_recent'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c486e",
   "metadata": {
    "papermill": {
     "duration": 0.018003,
     "end_time": "2022-11-02T13:14:20.574123",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.556120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f0cecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.613508Z",
     "iopub.status.busy": "2022-11-02T13:14:20.613108Z",
     "iopub.status.idle": "2022-11-02T13:14:20.620774Z",
     "shell.execute_reply": "2022-11-02T13:14:20.619400Z"
    },
    "papermill": {
     "duration": 0.030294,
     "end_time": "2022-11-02T13:14:20.622822",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.592528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Menentukan kolom berdasarkan Categoric dan Numeric\n",
    "kolom_categoric = [column for column, is_type in (df_temp.dtypes==\"object\").items() if is_type]\n",
    "kolom_numeric = [c for c in df_temp.columns if df_temp[c].dtype in ['int64', 'float64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc3b3c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.660761Z",
     "iopub.status.busy": "2022-11-02T13:14:20.660358Z",
     "iopub.status.idle": "2022-11-02T13:14:20.667299Z",
     "shell.execute_reply": "2022-11-02T13:14:20.666161Z"
    },
    "papermill": {
     "duration": 0.028625,
     "end_time": "2022-11-02T13:14:20.669461",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.640836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'customer_id',\n",
       " 'order_status',\n",
       " 'customer_unique_id',\n",
       " 'customer_city',\n",
       " 'customer_state',\n",
       " 'payment_type',\n",
       " 'product_id',\n",
       " 'seller_id',\n",
       " 'shipping_limit_date']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolom_categoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9029ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.707989Z",
     "iopub.status.busy": "2022-11-02T13:14:20.707477Z",
     "iopub.status.idle": "2022-11-02T13:14:20.714705Z",
     "shell.execute_reply": "2022-11-02T13:14:20.713567Z"
    },
    "papermill": {
     "duration": 0.029558,
     "end_time": "2022-11-02T13:14:20.717232",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.687674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_zip_code_prefix',\n",
       " 'payment_sequential',\n",
       " 'payment_installments',\n",
       " 'payment_value',\n",
       " 'order_item_id',\n",
       " 'price',\n",
       " 'freight_value',\n",
       " 'recency',\n",
       " 'frequency',\n",
       " 'monetary']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolom_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60421f6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:20.756021Z",
     "iopub.status.busy": "2022-11-02T13:14:20.755506Z",
     "iopub.status.idle": "2022-11-02T13:14:21.200388Z",
     "shell.execute_reply": "2022-11-02T13:14:21.198903Z"
    },
    "papermill": {
     "duration": 0.468452,
     "end_time": "2022-11-02T13:14:21.204314",
     "exception": false,
     "start_time": "2022-11-02T13:14:20.735862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- order_id ----------- \n",
      "\n",
      "895ab968e7bb0d5659d16cd74cd1650c    63\n",
      "fedcd9f7ccdc8cba3a18defedd1a5547    38\n",
      "fa65dad1b0e818e3ccc5cb0e39231352    29\n",
      "ccf804e764ed5650cd8759557269dc13    26\n",
      "a3725dfe487d359b5be08cac48b64ec5    24\n",
      "                                    ..\n",
      "f2b0b1751d796ee3bf5df90cb7c1c213     1\n",
      "2eadb8e36b6d6465dc9e2e3f3b95751d     1\n",
      "1373f8a3742861c26892cc2aab7c41e1     1\n",
      "1e6df4ea0f78bcf1e9e1a3abf97f28e1     1\n",
      "66dea50a8b16d9b4dee7af250b4be1a5     1\n",
      "Name: order_id, Length: 98665, dtype: int64\n",
      "----------- customer_id ----------- \n",
      "\n",
      "270c23a11d024a44c896d1894b261a83    63\n",
      "13aa59158da63ba0e93ec6ac2c07aacb    38\n",
      "9af2372a1e49340278e7c1ef8d749f34    29\n",
      "92cd3ec6e2d643d4ebd0e3d6238f69e2    26\n",
      "d22f25a9fadfb1abbc2e29395b1239f4    24\n",
      "                                    ..\n",
      "fc329f76b6bbfcdb6faafd1ec6c2dcc2     1\n",
      "a71faac9f56802e89e3ae192fd53f1af     1\n",
      "630b189fb0c73900cf3360c4a795680e     1\n",
      "462903f7a882d16b16e174dfec5aece4     1\n",
      "edb027a75a1449115f6b43211ae02a24     1\n",
      "Name: customer_id, Length: 98665, dtype: int64\n",
      "----------- order_status ----------- \n",
      "\n",
      "delivered      115035\n",
      "shipped          1244\n",
      "canceled          566\n",
      "processing        375\n",
      "invoiced          371\n",
      "unavailable         7\n",
      "approved            3\n",
      "Name: order_status, dtype: int64\n",
      "----------- order_purchase_timestamp ----------- \n",
      "\n",
      "2017-08-08 20:26:31    63\n",
      "2017-09-23 14:56:45    38\n",
      "2017-04-20 12:45:34    29\n",
      "2017-06-07 12:05:10    26\n",
      "2017-07-07 14:55:43    24\n",
      "                       ..\n",
      "2017-09-12 13:52:02     1\n",
      "2018-08-15 15:12:00     1\n",
      "2017-09-12 17:40:49     1\n",
      "2017-06-25 21:03:37     1\n",
      "2018-03-08 20:57:30     1\n",
      "Name: order_purchase_timestamp, Length: 98111, dtype: int64\n",
      "----------- order_approved_at ----------- \n",
      "\n",
      "2017-08-08 20:43:31    78\n",
      "2017-09-25 17:44:41    38\n",
      "2017-04-22 09:10:13    29\n",
      "2017-06-09 16:15:08    26\n",
      "2018-02-21 12:28:15    24\n",
      "                       ..\n",
      "2018-04-11 15:12:10     1\n",
      "2018-05-25 01:35:11     1\n",
      "2018-06-24 08:16:35     1\n",
      "2017-05-11 09:25:28     1\n",
      "2018-03-09 11:20:28     1\n",
      "Name: order_approved_at, Length: 90173, dtype: int64\n",
      "----------- order_delivered_carrier_date ----------- \n",
      "\n",
      "2017-08-10 11:58:14    1308\n",
      "2018-05-09 15:48:00      48\n",
      "2017-10-02 23:47:54      38\n",
      "2018-05-10 18:29:00      36\n",
      "2018-05-14 14:25:00      32\n",
      "                       ... \n",
      "2018-01-12 20:51:50       1\n",
      "2018-04-23 10:32:32       1\n",
      "2018-01-27 16:46:35       1\n",
      "2018-04-13 16:46:45       1\n",
      "2018-03-09 22:11:59       1\n",
      "Name: order_delivered_carrier_date, Length: 81016, dtype: int64\n",
      "----------- order_delivered_customer_date ----------- \n",
      "\n",
      "2017-08-14 12:46:18    2630\n",
      "2017-10-18 22:35:50      38\n",
      "2017-06-22 16:04:46      26\n",
      "2018-02-28 20:09:19      24\n",
      "2017-03-21 13:32:45      24\n",
      "                       ... \n",
      "2018-08-04 16:58:31       1\n",
      "2017-12-18 23:29:14       1\n",
      "2018-01-24 19:46:34       1\n",
      "2018-01-09 22:36:02       1\n",
      "2018-03-16 13:08:30       1\n",
      "Name: order_delivered_customer_date, Length: 95663, dtype: int64\n",
      "----------- order_estimated_delivery_date ----------- \n",
      "\n",
      "2017-12-20    649\n",
      "2018-03-12    611\n",
      "2018-05-29    609\n",
      "2018-03-13    607\n",
      "2018-07-16    590\n",
      "             ... \n",
      "2016-12-30      1\n",
      "2016-10-28      1\n",
      "2017-01-19      1\n",
      "2017-01-09      1\n",
      "2016-10-27      1\n",
      "Name: order_estimated_delivery_date, Length: 449, dtype: int64\n",
      "----------- customer_unique_id ----------- \n",
      "\n",
      "9a736b248f67d166d2fbb006bcb877c3    75\n",
      "6fbc7cdadbb522125f4b27ae9dee4060    38\n",
      "f9ae226291893fda10af7965268fb7f6    35\n",
      "8af7ac63b2efbcbd88e5b11505e8098a    29\n",
      "569aa12b73b5f7edeaa6f2a01603e381    26\n",
      "                                    ..\n",
      "0c4849ec22b169a6ed1b74d50dabbe0a     1\n",
      "fc91d9c645bd20c43a6c4589d21cbe17     1\n",
      "990e718a0f1677e3e5bb21eae3aa6b65     1\n",
      "1162303d63c2a9ad69b51ac7c75920f6     1\n",
      "60350aa974b26ff12caad89e55993bd6     1\n",
      "Name: customer_unique_id, Length: 95419, dtype: int64\n",
      "----------- customer_zip_code_prefix ----------- \n",
      "\n",
      "24220    158\n",
      "22790    155\n",
      "22793    154\n",
      "24230    138\n",
      "22775    127\n",
      "        ... \n",
      "77370      1\n",
      "26660      1\n",
      "78711      1\n",
      "72427      1\n",
      "45920      1\n",
      "Name: customer_zip_code_prefix, Length: 14976, dtype: int64\n",
      "----------- customer_city ----------- \n",
      "\n",
      "sao paulo                   18590\n",
      "rio de janeiro               8202\n",
      "belo horizonte               3247\n",
      "brasilia                     2457\n",
      "curitiba                     1809\n",
      "                            ...  \n",
      "miravania                       1\n",
      "santa rita da floresta          1\n",
      "pratapolis                      1\n",
      "vargem grande do soturno        1\n",
      "nova vicosa                     1\n",
      "Name: customer_city, Length: 4110, dtype: int64\n",
      "----------- customer_state ----------- \n",
      "\n",
      "SP    49566\n",
      "RJ    15327\n",
      "MG    13638\n",
      "RS     6486\n",
      "PR     5962\n",
      "SC     4302\n",
      "BA     4048\n",
      "DF     2473\n",
      "GO     2430\n",
      "ES     2338\n",
      "PE     1889\n",
      "CE     1551\n",
      "MT     1125\n",
      "PA     1116\n",
      "MA      844\n",
      "MS      843\n",
      "PB      639\n",
      "PI      573\n",
      "RN      569\n",
      "AL      458\n",
      "SE      397\n",
      "TO      339\n",
      "RO      286\n",
      "AM      171\n",
      "AC       95\n",
      "AP       84\n",
      "RR       52\n",
      "Name: customer_state, dtype: int64\n",
      "----------- payment_sequential ----------- \n",
      "\n",
      "1     112558\n",
      "2       3371\n",
      "3        644\n",
      "4        312\n",
      "5        187\n",
      "6        130\n",
      "7         89\n",
      "8         58\n",
      "9         48\n",
      "10        40\n",
      "11        35\n",
      "12        27\n",
      "13        16\n",
      "14        13\n",
      "15        11\n",
      "16         9\n",
      "18         9\n",
      "17         9\n",
      "19         9\n",
      "21         6\n",
      "20         6\n",
      "22         3\n",
      "25         2\n",
      "26         2\n",
      "23         2\n",
      "24         2\n",
      "27         1\n",
      "29         1\n",
      "28         1\n",
      "Name: payment_sequential, dtype: int64\n",
      "----------- payment_type ----------- \n",
      "\n",
      "credit_card    86769\n",
      "boleto         22867\n",
      "voucher         6274\n",
      "debit_card      1691\n",
      "Name: payment_type, dtype: int64\n",
      "----------- payment_installments ----------- \n",
      "\n",
      "1     58617\n",
      "2     13722\n",
      "3     11756\n",
      "4      7979\n",
      "10     6845\n",
      "5      6017\n",
      "8      5063\n",
      "6      4617\n",
      "7      1828\n",
      "9       726\n",
      "12      163\n",
      "15       92\n",
      "18       38\n",
      "24       34\n",
      "11       25\n",
      "20       21\n",
      "13       18\n",
      "14       16\n",
      "17        7\n",
      "16        7\n",
      "21        5\n",
      "0         3\n",
      "23        1\n",
      "22        1\n",
      "Name: payment_installments, dtype: int64\n",
      "----------- payment_value ----------- \n",
      "\n",
      "50.00      350\n",
      "100.00     299\n",
      "20.00      285\n",
      "77.57      250\n",
      "35.00      163\n",
      "          ... \n",
      "211.47       1\n",
      "1775.26      1\n",
      "520.29       1\n",
      "224.50       1\n",
      "222.84       1\n",
      "Name: payment_value, Length: 28938, dtype: int64\n",
      "----------- order_item_id ----------- \n",
      "\n",
      "1     103056\n",
      "2      10238\n",
      "3       2375\n",
      "4        986\n",
      "5        469\n",
      "6        262\n",
      "7         60\n",
      "8         36\n",
      "9         28\n",
      "10        25\n",
      "11        17\n",
      "12        13\n",
      "13         8\n",
      "14         7\n",
      "15         5\n",
      "16         3\n",
      "17         3\n",
      "18         3\n",
      "19         3\n",
      "20         3\n",
      "21         1\n",
      "Name: order_item_id, dtype: int64\n",
      "----------- product_id ----------- \n",
      "\n",
      "aca2eb7d00ea1a7b8ebd4e68314663af    536\n",
      "99a4788cb24856965c36a24e339b6058    525\n",
      "422879e10f46682990de24d770e7f83d    505\n",
      "389d119b48cf3043d311335e499d9c6b    406\n",
      "368c6c730842d78016ad823897a372db    395\n",
      "                                   ... \n",
      "45cc9e057103e5e2176034fec87a06ca      1\n",
      "e189be46c3e52f23ebe8a520e59d2c46      1\n",
      "264aad5ca689ed34e57633f78c5683b4      1\n",
      "2f13610363de5834c81405b88f4b8b57      1\n",
      "006619bbed68b000c8ba3f8725d5409e      1\n",
      "Name: product_id, Length: 32951, dtype: int64\n",
      "----------- seller_id ----------- \n",
      "\n",
      "4a3ca9315b744ce9f8e9374361493884    2133\n",
      "6560211a19b47992c3666cc44a7e94c0    2122\n",
      "1f50f920176fa81dab994f9023523100    2008\n",
      "cc419e0650a3c5ba77189a1882b7556a    1847\n",
      "da8622b14eb17ae2831f4ac5b9dab84a    1639\n",
      "                                    ... \n",
      "a150c540f572d0fb53992264bc5c10b5       1\n",
      "4ce6e5f6c52515177e18c1c9361d8677       1\n",
      "b3809b8d4394588a47c8470ea331e135       1\n",
      "f90f77ef2799a27f80d90c425ca944f7       1\n",
      "f3862c2188522d89860c38a3ea8b550d       1\n",
      "Name: seller_id, Length: 3095, dtype: int64\n",
      "----------- shipping_limit_date ----------- \n",
      "\n",
      "2017-08-14 20:43:31    63\n",
      "2017-10-05 17:44:41    38\n",
      "2017-04-27 09:10:13    29\n",
      "2017-06-15 16:15:08    26\n",
      "2018-02-27 12:28:15    24\n",
      "                       ..\n",
      "2018-08-01 02:10:33     1\n",
      "2017-10-04 10:28:21     1\n",
      "2017-05-31 23:50:14     1\n",
      "2017-11-23 01:31:00     1\n",
      "2018-03-15 10:55:42     1\n",
      "Name: shipping_limit_date, Length: 93317, dtype: int64\n",
      "----------- price ----------- \n",
      "\n",
      "59.90     2606\n",
      "69.90     2092\n",
      "49.90     2045\n",
      "89.90     1623\n",
      "99.90     1510\n",
      "          ... \n",
      "108.50       1\n",
      "20.94        1\n",
      "31.08        1\n",
      "86.98        1\n",
      "213.39       1\n",
      "Name: price, Length: 5968, dtype: int64\n",
      "----------- freight_value ----------- \n",
      "\n",
      "15.10    3831\n",
      "7.78     2339\n",
      "11.85    1972\n",
      "14.10    1971\n",
      "18.23    1625\n",
      "         ... \n",
      "88.24       1\n",
      "80.56       1\n",
      "92.01       1\n",
      "73.05       1\n",
      "36.89       1\n",
      "Name: freight_value, Length: 6999, dtype: int64\n",
      "----------- most_recent ----------- \n",
      "\n",
      "2017-08-08 20:43:31    78\n",
      "2017-09-25 17:44:41    38\n",
      "2017-04-22 09:10:13    29\n",
      "2017-06-09 16:15:08    26\n",
      "2018-02-21 12:28:15    24\n",
      "                       ..\n",
      "2018-04-11 15:12:10     1\n",
      "2018-05-25 01:35:11     1\n",
      "2018-06-24 08:16:35     1\n",
      "2017-05-11 09:25:28     1\n",
      "2018-03-09 11:20:28     1\n",
      "Name: most_recent, Length: 90173, dtype: int64\n",
      "----------- recency ----------- \n",
      "\n",
      "131.0    1285\n",
      "282.0    1106\n",
      "60.0      761\n",
      "283.0     759\n",
      "279.0     635\n",
      "         ... \n",
      "603.0       2\n",
      "606.0       2\n",
      "618.0       1\n",
      "0.0         1\n",
      "690.0       1\n",
      "Name: recency, Length: 612, dtype: int64\n",
      "----------- frequency ----------- \n",
      "\n",
      "1     86189\n",
      "2     18854\n",
      "3      4698\n",
      "4      3116\n",
      "6      1620\n",
      "5      1250\n",
      "7       329\n",
      "8       256\n",
      "12      240\n",
      "10      150\n",
      "11      132\n",
      "9       108\n",
      "24       96\n",
      "15       90\n",
      "14       70\n",
      "21       63\n",
      "63       63\n",
      "20       60\n",
      "13       52\n",
      "38       38\n",
      "29       29\n",
      "26       26\n",
      "22       22\n",
      "19       19\n",
      "16       16\n",
      "0        15\n",
      "Name: frequency, dtype: int64\n",
      "----------- monetary ----------- \n",
      "\n",
      "77.57      259\n",
      "35.00      183\n",
      "73.34      168\n",
      "116.94     132\n",
      "107.78     125\n",
      "          ... \n",
      "342.25       1\n",
      "415.51       1\n",
      "1023.37      1\n",
      "139.47       1\n",
      "99.83        1\n",
      "Name: monetary, Length: 30065, dtype: int64\n",
      "----------- is_churn ----------- \n",
      "\n",
      "False    89627\n",
      "True     27974\n",
      "Name: is_churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Melihat Frequency Unique Values di Masing Masing Kolom\n",
    "for nama_kolom in df_temp.columns:\n",
    "    print(f\"----------- {nama_kolom} ----------- \\n\")\n",
    "    print(df_temp[nama_kolom].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ad6736b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:21.243997Z",
     "iopub.status.busy": "2022-11-02T13:14:21.243543Z",
     "iopub.status.idle": "2022-11-02T13:14:21.424511Z",
     "shell.execute_reply": "2022-11-02T13:14:21.423220Z"
    },
    "papermill": {
     "duration": 0.20413,
     "end_time": "2022-11-02T13:14:21.427328",
     "exception": false,
     "start_time": "2022-11-02T13:14:21.223198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label Encoding Kolom Payment Type\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df_temp['payment_type'] = labelencoder.fit_transform(df_temp['payment_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "272e3c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:21.467405Z",
     "iopub.status.busy": "2022-11-02T13:14:21.466395Z",
     "iopub.status.idle": "2022-11-02T13:14:21.511547Z",
     "shell.execute_reply": "2022-11-02T13:14:21.510406Z"
    },
    "papermill": {
     "duration": 0.06819,
     "end_time": "2022-11-02T13:14:21.514366",
     "exception": false,
     "start_time": "2022-11-02T13:14:21.446176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label Encoding Kolom Customer_city\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df_temp['customer_city'] = labelencoder.fit_transform(df_temp['customer_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5c28e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:21.554075Z",
     "iopub.status.busy": "2022-11-02T13:14:21.553607Z",
     "iopub.status.idle": "2022-11-02T13:14:21.655668Z",
     "shell.execute_reply": "2022-11-02T13:14:21.654252Z"
    },
    "papermill": {
     "duration": 0.124938,
     "end_time": "2022-11-02T13:14:21.658184",
     "exception": false,
     "start_time": "2022-11-02T13:14:21.533246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_city</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>price</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3588</td>\n",
       "      <td>1</td>\n",
       "      <td>29.99</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3588</td>\n",
       "      <td>3</td>\n",
       "      <td>29.99</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3588</td>\n",
       "      <td>3</td>\n",
       "      <td>29.99</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>141.46</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4041</td>\n",
       "      <td>1</td>\n",
       "      <td>159.90</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>179.12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117596</th>\n",
       "      <td>2995</td>\n",
       "      <td>1</td>\n",
       "      <td>174.90</td>\n",
       "      <td>209.0</td>\n",
       "      <td>1</td>\n",
       "      <td>195.00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117597</th>\n",
       "      <td>2537</td>\n",
       "      <td>1</td>\n",
       "      <td>205.99</td>\n",
       "      <td>372.0</td>\n",
       "      <td>1</td>\n",
       "      <td>271.01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117598</th>\n",
       "      <td>1928</td>\n",
       "      <td>1</td>\n",
       "      <td>179.99</td>\n",
       "      <td>237.0</td>\n",
       "      <td>2</td>\n",
       "      <td>882.32</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117599</th>\n",
       "      <td>1928</td>\n",
       "      <td>1</td>\n",
       "      <td>179.99</td>\n",
       "      <td>237.0</td>\n",
       "      <td>2</td>\n",
       "      <td>882.32</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117600</th>\n",
       "      <td>2067</td>\n",
       "      <td>2</td>\n",
       "      <td>68.50</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1</td>\n",
       "      <td>86.86</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117601 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer_city  payment_type   price  recency  frequency  monetary  \\\n",
       "0                3588             1   29.99    336.0          3     38.71   \n",
       "1                3588             3   29.99    336.0          3     38.71   \n",
       "2                3588             3   29.99    336.0          3     38.71   \n",
       "3                 417             0  118.70     39.0          1    141.46   \n",
       "4                4041             1  159.90     26.0          1    179.12   \n",
       "...               ...           ...     ...      ...        ...       ...   \n",
       "117596           2995             1  174.90    209.0          1    195.00   \n",
       "117597           2537             1  205.99    372.0          1    271.01   \n",
       "117598           1928             1  179.99    237.0          2    882.32   \n",
       "117599           1928             1  179.99    237.0          2    882.32   \n",
       "117600           2067             2   68.50    178.0          1     86.86   \n",
       "\n",
       "        is_churn  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  \n",
       "...          ...  \n",
       "117596     False  \n",
       "117597      True  \n",
       "117598     False  \n",
       "117599     False  \n",
       "117600     False  \n",
       "\n",
       "[117601 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Columns\n",
    "df_temp.drop(columns=['order_id', 'customer_id', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'customer_zip_code_prefix', 'order_estimated_delivery_date', 'customer_unique_id', 'seller_id', 'payment_sequential', 'order_purchase_timestamp', 'customer_state', 'order_approved_at', 'order_status', 'payment_installments', 'order_item_id', 'product_id', 'shipping_limit_date', 'payment_value', 'freight_value', 'most_recent'], inplace = True)\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca83d0b",
   "metadata": {
    "papermill": {
     "duration": 0.018667,
     "end_time": "2022-11-02T13:14:21.696040",
     "exception": false,
     "start_time": "2022-11-02T13:14:21.677373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be9b348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:21.736137Z",
     "iopub.status.busy": "2022-11-02T13:14:21.735722Z",
     "iopub.status.idle": "2022-11-02T13:14:22.587664Z",
     "shell.execute_reply": "2022-11-02T13:14:22.586336Z"
    },
    "papermill": {
     "duration": 0.875303,
     "end_time": "2022-11-02T13:14:22.590243",
     "exception": false,
     "start_time": "2022-11-02T13:14:21.714940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cek outlier Box Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "sns.boxplot(data=df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98a7c717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:22.631686Z",
     "iopub.status.busy": "2022-11-02T13:14:22.630690Z",
     "iopub.status.idle": "2022-11-02T13:14:22.647516Z",
     "shell.execute_reply": "2022-11-02T13:14:22.646146Z"
    },
    "papermill": {
     "duration": 0.040255,
     "end_time": "2022-11-02T13:14:22.650186",
     "exception": false,
     "start_time": "2022-11-02T13:14:22.609931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Z-Score\n",
    "columns = ['monetary', 'price']\n",
    "\n",
    "for i in columns:\n",
    "    upper_limit = df_temp[i].mean() + 3*df_temp[i].std()\n",
    "    lower_limit = df_temp[i].mean() - 3*df_temp[i].std()\n",
    "\n",
    "    df_temp[i] = np.where(\n",
    "        df_temp[i]>upper_limit,\n",
    "        upper_limit,\n",
    "        np.where(\n",
    "            df_temp[i]<lower_limit,\n",
    "            lower_limit,\n",
    "            df_temp[i]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61395654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:22.691525Z",
     "iopub.status.busy": "2022-11-02T13:14:22.690719Z",
     "iopub.status.idle": "2022-11-02T13:14:23.565014Z",
     "shell.execute_reply": "2022-11-02T13:14:23.564149Z"
    },
    "papermill": {
     "duration": 0.897313,
     "end_time": "2022-11-02T13:14:23.567301",
     "exception": false,
     "start_time": "2022-11-02T13:14:22.669988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cek Kembali Outlier Box Plot yang telah dibuang outliernya menggunakan Z-Score\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "sns.boxplot(data=df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c9b4e",
   "metadata": {
    "papermill": {
     "duration": 0.019946,
     "end_time": "2022-11-02T13:14:23.607260",
     "exception": false,
     "start_time": "2022-11-02T13:14:23.587314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cek Data Balnce atau Tidak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2de80b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:23.649884Z",
     "iopub.status.busy": "2022-11-02T13:14:23.649211Z",
     "iopub.status.idle": "2022-11-02T13:14:23.844457Z",
     "shell.execute_reply": "2022-11-02T13:14:23.843535Z"
    },
    "papermill": {
     "duration": 0.218945,
     "end_time": "2022-11-02T13:14:23.846677",
     "exception": false,
     "start_time": "2022-11-02T13:14:23.627732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='is_churn', ylabel='count'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAF0CAYAAAC64eqOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAU/UlEQVR4nO3dfbDmZX3f8c9XVnyMCrIxytIsU2lTYpuKO4Tq1CbBIpoHbEatVgOxTOhMMDFtk1Y7TnFM6NTWxGhqbImg4DjBx0TSkFIGtU5bQRalQTDGHXwARmUjqNFUdMm3f5xrm9vl7O7h4T6Hc67Xa+bMuX/X7/rd57r/OHve+7sfftXdAQDm85CNXgAAsDFEAABMSgQAwKREAABMSgQAwKREAABMattGL2C9HXPMMb1z586NXgYArIvrrrvuz7p7+2r7pouAnTt3Zvfu3Ru9DABYF1X1+YPt83QAAExKBADApEQAAExKBADApEQAAExKBADApEQAAExKBADApEQAAExKBADApEQAAExKBADApEQAAExquqsILsvTfuWSjV4C3G/X/cczN3oJwDpyJgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJiUCAGBSIgAAJrXUCKiqf15VN1bVJ6vqd6vq4VV1fFVdU1V7qupdVXXkmPuwsb1n7N+5cD+vGuOfrqpnL4yfPsb2VNUrl/lYAGCrWVoEVNWxSX4xya7ufkqSI5K8KMnrkryhu5+c5M4kZ49Dzk5y5xh/w5iXqjpxHPeDSU5P8ttVdURVHZHkzUmek+TEJC8ecwGANVj20wHbkjyiqrYleWSSLyb5sSTvHfsvTvK8cfuMsZ2x/9SqqjF+aXff1d2fTbInycnja09339zd305y6ZgLAKzB0iKgu29L8vokX8jKH/+vJbkuyVe7e9+YdmuSY8ftY5PcMo7dN+Y/fnH8gGMONg4ArMEynw44Kiv/Mz8+yZOSPCorp/PXXVWdU1W7q2r33r17N2IJAPCgs8ynA56V5LPdvbe7v5Pk/UmekeRx4+mBJNmR5LZx+7YkxyXJ2P/YJF9ZHD/gmION30N3X9Ddu7p71/bt2x+IxwYAm94yI+ALSU6pqkeO5/ZPTXJTkg8lef6Yc1aSD4zbl43tjP0f7O4e4y8a7x44PskJST6W5NokJ4x3GxyZlRcPXrbExwMAW8q2w0+5b7r7mqp6b5KPJ9mX5BNJLkjyh0kurapfG2MXjkMuTPKOqtqT5I6s/FFPd99YVe/OSkDsS3Jud9+dJFX18iRXZOWdBxd1943LejwAsNUsLQKSpLvPS3LeAcM3Z+WV/QfO/VaSFxzkfs5Pcv4q45cnufz+rxQA5uMTAwFgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACa11AioqsdV1Xur6k+q6lNV9feq6uiqurKqPjO+HzXmVlW9qar2VNUfV9VJC/dz1pj/mao6a2H8aVV1wzjmTVVVy3w8ALCVLPtMwBuT/Lfu/oEkP5TkU0lemeSq7j4hyVVjO0mek+SE8XVOkrckSVUdneS8JD+c5OQk5+0PhzHn5xaOO33JjwcAtoylRUBVPTbJM5NcmCTd/e3u/mqSM5JcPKZdnOR54/YZSS7pFVcneVxVPTHJs5Nc2d13dPedSa5McvrY95juvrq7O8klC/cFABzGMs8EHJ9kb5K3VdUnquqtVfWoJE/o7i+OOV9K8oRx+9gktywcf+sYO9T4rauMAwBrsMwI2JbkpCRv6e6nJvlm/urUf5Jk/A++l7iGJElVnVNVu6tq9969e5f94wBgU1hmBNya5NbuvmZsvzcrUfDlcSo/4/vtY/9tSY5bOH7HGDvU+I5Vxu+huy/o7l3dvWv79u3360EBwFaxtAjo7i8luaWq/uYYOjXJTUkuS7L/Ff5nJfnAuH1ZkjPHuwROSfK18bTBFUlOq6qjxgsCT0tyxdj39ao6Zbwr4MyF+wIADmPbku//F5K8s6qOTHJzkpdlJTzeXVVnJ/l8kheOuZcneW6SPUn+YsxNd99RVb+a5Nox77Xdfce4/fNJ3p7kEUn+aHwBAGuw1Ajo7uuT7Fpl16mrzO0k5x7kfi5KctEq47uTPOX+rRIA5uQTAwFgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACYlAgBgUiIAACa1pgioqqvWMgYAbB7bDrWzqh6e5JFJjqmqo5LU2PWYJMcueW0AwBIdMgKS/LMkv5TkSUmuy19FwNeT/KflLQsAWLZDRkB3vzHJG6vqF7r7t9ZpTQDAOjjcmYAkSXf/VlU9PcnOxWO6+5IlrQsAWLI1RUBVvSPJX09yfZK7x3AnEQEAsEmtKQKS7EpyYnf3MhcDAKyftX5OwCeTfN8yFwIArK+1ngk4JslNVfWxJHftH+zun1rKqgCApVtrBLxmmYsAANbfWt8d8D+WvRAAYH2t9d0Bf56VdwMkyZFJHprkm939mGUtDABYrrWeCfie/berqpKckeSUZS0KAFi+e30VwV7x+0me/cAvBwBYL2t9OuCnFzYfkpXPDfjWUlYEAKyLtb474CcXbu9L8rmsPCUAAGxSa31NwMuWvRAAYH2t6TUBVbWjqn6vqm4fX++rqh3LXhwAsDxrfWHg25JcluRJ4+sPxhgAsEmtNQK2d/fbunvf+Hp7ku1LXBcAsGRrjYCvVNVLq+qI8fXSJF9Z5sIAgOVaawT80yQvTPKlJF9M8vwkP7ukNQEA62CtbxF8bZKzuvvOJKmqo5O8PitxAABsQms9E/B39gdAknT3HUmeupwlAQDrYa0R8JCqOmr/xjgTsNazCADAg9Ba/5D/epKPVtV7xvYLkpy/nCUBAOthrZ8YeElV7U7yY2Pop7v7puUtCwBYtjWf0h9/9P3hB4At4l5fShgA2BpEAABMSgQAwKREAABMSgQAwKSWHgHjgkOfqKr/OraPr6prqmpPVb2rqo4c4w8b23vG/p0L9/GqMf7pqnr2wvjpY2xPVb1y2Y8FALaS9TgT8Iokn1rYfl2SN3T3k5PcmeTsMX52kjvH+BvGvFTViUlelOQHk5ye5Lf3X80wyZuTPCfJiUlePOYCAGuw1Aioqh1JfjzJW8d2ZeUDh947plyc5Hnj9hljO2P/qWP+GUku7e67uvuzSfYkOXl87enum7v720kuHXMBgDVY9pmA30zyr5L85dh+fJKvdve+sX1rkmPH7WOT3JIkY//Xxvz/P37AMQcbv4eqOqeqdlfV7r17997PhwQAW8PSIqCqfiLJ7d193bJ+xlp19wXdvau7d23fvn2jlwMADwrLvBLgM5L8VFU9N8nDkzwmyRuTPK6qto3/7e9IctuYf1uS45LcWlXbkjw2yVcWxvdbPOZg4wDAYSztTEB3v6q7d3T3zqy8sO+D3f2SJB9K8vwx7awkHxi3LxvbGfs/2N09xl803j1wfJITknwsybVJThjvNjhy/IzLlvV4AGCrWeaZgIP510kurapfS/KJJBeO8QuTvKOq9iS5Iyt/1NPdN1bVu7Ny8aJ9Sc7t7ruTpKpenuSKJEckuai7b1zXRwIAm9i6REB3fzjJh8ftm7Pyyv4D53wryQsOcvz5Sc5fZfzyJJc/gEsFgGn4xEAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJiQAAmJQIAIBJbdvoBQDcH1947d/e6CXAA+Kv/dsb1v1nOhMAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJNaWgRU1XFV9aGquqmqbqyqV4zxo6vqyqr6zPh+1BivqnpTVe2pqj+uqpMW7uusMf8zVXXWwvjTquqGccybqqqW9XgAYKtZ5pmAfUn+ZXefmOSUJOdW1YlJXpnkqu4+IclVYztJnpPkhPF1TpK3JCvRkOS8JD+c5OQk5+0PhzHn5xaOO32JjwcAtpSlRUB3f7G7Pz5u/3mSTyU5NskZSS4e0y5O8rxx+4wkl/SKq5M8rqqemOTZSa7s7ju6+84kVyY5fex7THdf3d2d5JKF+wIADmNdXhNQVTuTPDXJNUme0N1fHLu+lOQJ4/axSW5ZOOzWMXao8VtXGQcA1mDpEVBVj07yviS/1N1fX9w3/gff67CGc6pqd1Xt3rt377J/HABsCkuNgKp6aFYC4J3d/f4x/OVxKj/j++1j/LYkxy0cvmOMHWp8xyrj99DdF3T3ru7etX379vv3oABgi1jmuwMqyYVJPtXdv7Gw67Ik+1/hf1aSDyyMnzneJXBKkq+Npw2uSHJaVR01XhB4WpIrxr6vV9Up42eduXBfAMBhbFvifT8jyc8kuaGqrh9j/ybJv0/y7qo6O8nnk7xw7Ls8yXOT7EnyF0leliTdfUdV/WqSa8e813b3HeP2zyd5e5JHJPmj8QUArMHSIqC7/2eSg71v/9RV5neScw9yXxcluWiV8d1JnnI/lgkA0/KJgQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJMSAQAwKREAAJPa9BFQVadX1aerak9VvXKj1wMAm8WmjoCqOiLJm5M8J8mJSV5cVSdu7KoAYHPY1BGQ5OQke7r75u7+dpJLk5yxwWsCgE1hs0fAsUluWdi+dYwBAIexbaMXsB6q6pwk54zNb1TVpzdyPdxnxyT5s41exFZWrz9ro5fAg5PfvfVwXi3rnr//YDs2ewTcluS4he0dY+y7dPcFSS5Yr0WxHFW1u7t3bfQ6YDZ+97auzf50wLVJTqiq46vqyCQvSnLZBq8JADaFTX0moLv3VdXLk1yR5IgkF3X3jRu8LADYFDZ1BCRJd1+e5PKNXgfrwlM6sDH87m1R1d0bvQYAYANs9tcEAAD30aZ/OoDNraruTnLDwtDzuvtzB5n7je5+9LosDCZQVY9PctXY/L4kdyfZO7ZPHh/Cxhbm6QA21L35wy4CYHmq6jVJvtHdr18Y29bd+zZuVSybpwN4UKmqR1fVVVX18aq6oaru8THQVfXEqvpIVV1fVZ+sqr8/xk+rqo+OY99TVYIB7qWqentV/eequibJf6iq11TVLy/s/2RV7Ry3X1pVHxu/i/9lXM+FTUQEsNEeMf4Bub6qfi/Jt5L8o+4+KcmPJvn1qjrwY7T+SZIruvvvJvmhJNdX1TFJXp3kWePY3Un+xbo9CthadiR5encf9Heoqv5Wkn+c5Bnjd/HuJC9Zn+XxQPGaADba/x3/gCRJquqhSf5dVT0zyV9m5VoQT0jypYVjrk1y0Zj7+919fVX9g6xcSfJ/jWY4MslH1+chwJbznu6++zBzTk3ytCTXjt+5RyS5fdkL44ElAniweUmS7Ume1t3fqarPJXn44oTu/siIhB9P8vaq+o0kdya5srtfvN4Lhi3omwu39+W7zxrv/32sJBd396vWbVU84DwdwIPNY5PcPgLgR7PKhS+q6vuTfLm7fyfJW5OclOTqJM+oqiePOY+qqr+xjuuGrepzWfkdS1WdlOT4MX5VkudX1feOfUeP3002EWcCeLB5Z5I/qKobsvK8/p+sMudHkvxKVX0nyTeSnNnde6vqZ5P8blU9bMx7dZI/Xf6SYUt7X5Izq+rGJNdk/E51901V9eok/72qHpLkO0nOTfL5DVsp95q3CALApDwdAACTEgEAMCkRAACTEgEAMCkRAACTEgEAMCkRAKyqqv73A3Q/33UBGuDBQwQAq+rup2/0GpKVy9lu9BpgqxIBwKqq6hvj+6qXbj7IMaePSzn/n6q6amHXiVX14aq6uap+cczdWVWfXDj2l8c17TPm/mZV7U7yirH9unHZ2j891BqAtVPYwOHsv3Tz+eN68Y9cbVJVbU/yO0me2d2fraqjF3b/QFYuDf09ST5dVW9Zw889srt3jfv+ySTbuvvkqnpukvOSPOu+PyQgEQHA4d3j0s0HmXdKko9092eTpLvvWNj3h919V5K7qur2rFwe+nDedcD2+8f365LsXOPagUPwdABwSN39kSTPTHJbVi7dfOZ9uJu7Fm7fnZX/gBzsErX7ffOA7f33sf944H4SAcAhHeTSzau5Oskzq+r4cdzRB5m335eTfG9VPX5c+fEnHqg1A2ujpoHD+ZEccOnm1SaNyzmfk+T949Kytyf5hwe70+7+TlW9NsnHsnKWYbXLRgNL5FLCADApTwcAwKQ8HQDca1V1TZKHHTD8M919w0asB7hvPB0AAJPydAAATEoEAMCkRAAATEoEAMCkRAAATOr/AfkipjAcxsrwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imbalance Data\n",
    "\n",
    "# Observe imbalance data here \n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(df_temp.is_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc373f",
   "metadata": {
    "papermill": {
     "duration": 0.019994,
     "end_time": "2022-11-02T13:14:23.886998",
     "exception": false,
     "start_time": "2022-11-02T13:14:23.867004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Korelasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa37b0e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:23.929538Z",
     "iopub.status.busy": "2022-11-02T13:14:23.928893Z",
     "iopub.status.idle": "2022-11-02T13:14:24.365463Z",
     "shell.execute_reply": "2022-11-02T13:14:24.364534Z"
    },
    "papermill": {
     "duration": 0.460744,
     "end_time": "2022-11-02T13:14:24.367951",
     "exception": false,
     "start_time": "2022-11-02T13:14:23.907207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = df_temp.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(upper, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dd801",
   "metadata": {
    "papermill": {
     "duration": 0.02072,
     "end_time": "2022-11-02T13:14:24.409916",
     "exception": false,
     "start_time": "2022-11-02T13:14:24.389196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Memisahkan X dan y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c8c1f38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:24.454429Z",
     "iopub.status.busy": "2022-11-02T13:14:24.453673Z",
     "iopub.status.idle": "2022-11-02T13:14:24.470069Z",
     "shell.execute_reply": "2022-11-02T13:14:24.468861Z"
    },
    "papermill": {
     "duration": 0.041511,
     "end_time": "2022-11-02T13:14:24.472553",
     "exception": false,
     "start_time": "2022-11-02T13:14:24.431042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_city</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>price</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3588</td>\n",
       "      <td>1</td>\n",
       "      <td>29.99</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3588</td>\n",
       "      <td>3</td>\n",
       "      <td>29.99</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3588</td>\n",
       "      <td>3</td>\n",
       "      <td>29.99</td>\n",
       "      <td>336.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>141.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4041</td>\n",
       "      <td>1</td>\n",
       "      <td>159.90</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>179.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_city  payment_type   price  recency  frequency  monetary\n",
       "0           3588             1   29.99    336.0          3     38.71\n",
       "1           3588             3   29.99    336.0          3     38.71\n",
       "2           3588             3   29.99    336.0          3     38.71\n",
       "3            417             0  118.70     39.0          1    141.46\n",
       "4           4041             1  159.90     26.0          1    179.12"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memisahkan mana X dan mana y\n",
    "y= df_temp['is_churn']\n",
    "X= df_temp.drop(['is_churn'],1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4827ebef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:24.517146Z",
     "iopub.status.busy": "2022-11-02T13:14:24.516723Z",
     "iopub.status.idle": "2022-11-02T13:14:24.525486Z",
     "shell.execute_reply": "2022-11-02T13:14:24.524309Z"
    },
    "papermill": {
     "duration": 0.034007,
     "end_time": "2022-11-02T13:14:24.527776",
     "exception": false,
     "start_time": "2022-11-02T13:14:24.493769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "117596    False\n",
       "117597     True\n",
       "117598    False\n",
       "117599    False\n",
       "117600    False\n",
       "Name: is_churn, Length: 117601, dtype: bool"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc819e",
   "metadata": {
    "papermill": {
     "duration": 0.021949,
     "end_time": "2022-11-02T13:14:24.571369",
     "exception": false,
     "start_time": "2022-11-02T13:14:24.549420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Base Line Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c51b0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:24.617770Z",
     "iopub.status.busy": "2022-11-02T13:14:24.616532Z",
     "iopub.status.idle": "2022-11-02T13:14:25.988112Z",
     "shell.execute_reply": "2022-11-02T13:14:25.986796Z"
    },
    "papermill": {
     "duration": 1.398164,
     "end_time": "2022-11-02T13:14:25.991668",
     "exception": false,
     "start_time": "2022-11-02T13:14:24.593504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Machine learning Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70dc20cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:26.037190Z",
     "iopub.status.busy": "2022-11-02T13:14:26.036755Z",
     "iopub.status.idle": "2022-11-02T13:14:26.041715Z",
     "shell.execute_reply": "2022-11-02T13:14:26.040510Z"
    },
    "papermill": {
     "duration": 0.0306,
     "end_time": "2022-11-02T13:14:26.044177",
     "exception": false,
     "start_time": "2022-11-02T13:14:26.013577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cross Validation -> Untuk menangani data yang tidak balance\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64fd4922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:26.090341Z",
     "iopub.status.busy": "2022-11-02T13:14:26.089935Z",
     "iopub.status.idle": "2022-11-02T13:14:26.096045Z",
     "shell.execute_reply": "2022-11-02T13:14:26.094822Z"
    },
    "papermill": {
     "duration": 0.032569,
     "end_time": "2022-11-02T13:14:26.098745",
     "exception": false,
     "start_time": "2022-11-02T13:14:26.066176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import make_scorer,accuracy_score,roc_auc_score,precision_score,recall_score,f1_score,log_loss\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "736c1e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:14:26.145783Z",
     "iopub.status.busy": "2022-11-02T13:14:26.144891Z",
     "iopub.status.idle": "2022-11-02T13:26:53.075487Z",
     "shell.execute_reply": "2022-11-02T13:26:53.074155Z"
    },
    "papermill": {
     "duration": 746.957339,
     "end_time": "2022-11-02T13:26:53.079001",
     "exception": false,
     "start_time": "2022-11-02T13:14:26.121662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------BEFORE------------\n",
      "DecisionTreeClassifier Acc Train: [], 1 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [], 1 of KFold 5\n",
      "DecisionTreeClassifier Recall: [], 1 of KFold 5\n",
      "DecisionTreeClassifier Precission: [], 1 of KFold 5\n",
      "DecisionTreeClassifier AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "DecisionTreeClassifier Acc Train: [1.0], 1 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0], 1 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0], 1 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0], 1 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "DecisionTreeClassifier Acc Train: [1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "DecisionTreeClassifier Acc Train: [1.0, 1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0, 1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0, 1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0, 1.0], 2 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0, 1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "DecisionTreeClassifier Acc Train: [1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "DecisionTreeClassifier Acc Train: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "DecisionTreeClassifier Acc Train: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "DecisionTreeClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "DecisionTreeClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "DecisionTreeClassifier Acc Train: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier Acc Test: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier Recall: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier Precission: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "DecisionTreeClassifier AUC: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "DecisionTreeClassifier Acc Train: 1.0\n",
      "DecisionTreeClassifier Acc Test: 1.0\n",
      "DecisionTreeClassifier Recall: 1.0\n",
      "DecisionTreeClassifier Precission: 1.0\n",
      "DecisionTreeClassifier AUC: 1.0\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LogisticRegression Acc Train: [], 1 of KFold 5\n",
      "LogisticRegression Acc Test: [], 1 of KFold 5\n",
      "LogisticRegression Recall: [], 1 of KFold 5\n",
      "LogisticRegression Precission: [], 1 of KFold 5\n",
      "LogisticRegression AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925], 1 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535], 1 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148], 1 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307], 1 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925], 2 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535], 2 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148], 2 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307], 2 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925, 0.9262975521093526], 2 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535, 0.9281462585034014], 2 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148, 0.8832677869145513], 2 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307, 0.826530612244898], 2 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597, 0.9127094262030081], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925, 0.9262975521093526], 3 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535, 0.9281462585034014], 3 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148, 0.8832677869145513], 3 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307, 0.826530612244898], 3 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597, 0.9127094262030081], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925, 0.9262975521093526, 0.9870430798992358], 3 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535, 0.9281462585034014, 0.9866071428571429], 3 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148, 0.8832677869145513, 0.9848078641644326], 3 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307, 0.826530612244898, 0.9599303135888502], 3 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597, 0.9127094262030081, 0.985988311440654], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925, 0.9262975521093526, 0.9870430798992358], 4 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535, 0.9281462585034014, 0.9866071428571429], 4 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148, 0.8832677869145513, 0.9848078641644326], 4 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307, 0.826530612244898, 0.9599303135888502], 4 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597, 0.9127094262030081, 0.985988311440654], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925, 0.9262975521093526, 0.9870430798992358, 0.9445052667382362], 4 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535, 0.9281462585034014, 0.9866071428571429, 0.9444302721088436], 4 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148, 0.8832677869145513, 0.9848078641644326, 0.898302055406613], 4 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307, 0.826530612244898, 0.9599303135888502, 0.8719639139486468], 4 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597, 0.9127094262030081, 0.985988311440654, 0.9285652536447291], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925, 0.9262975521093526, 0.9870430798992358, 0.9445052667382362], 5 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535, 0.9281462585034014, 0.9866071428571429, 0.9444302721088436], 5 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148, 0.8832677869145513, 0.9848078641644326, 0.898302055406613], 5 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307, 0.826530612244898, 0.9599303135888502, 0.8719639139486468], 5 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597, 0.9127094262030081, 0.985988311440654, 0.9285652536447291], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LogisticRegression Acc Train: [0.9545705782312925, 0.9262975521093526, 0.9870430798992358, 0.9445052667382362, 0.9768816232820654], 5 of KFold 5\n",
      "LogisticRegression Acc Test: [0.9531482504995535, 0.9281462585034014, 0.9866071428571429, 0.9444302721088436, 0.9774659863945578], 5 of KFold 5\n",
      "LogisticRegression Recall: [0.9172475424486148, 0.8832677869145513, 0.9848078641644326, 0.898302055406613, 0.9403038427167113], 5 of KFold 5\n",
      "LogisticRegression Precission: [0.8892739559868307, 0.826530612244898, 0.9599303135888502, 0.8719639139486468, 0.9640828293934396], 5 of KFold 5\n",
      "LogisticRegression AUC: [0.9408004977667597, 0.9127094262030081, 0.985988311440654, 0.9285652536447291, 0.9646846968116332], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "LogisticRegression Acc Train: 0.9578596200520366\n",
      "LogisticRegression Acc Test: 0.9579595820726998\n",
      "LogisticRegression Recall: 0.9247858183301847\n",
      "LogisticRegression Precission: 0.9023563250325329\n",
      "LogisticRegression AUC: 0.9465496371733568\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "KNeighborsClassifier Acc Train: [], 1 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [], 1 of KFold 5\n",
      "KNeighborsClassifier Recall: [], 1 of KFold 5\n",
      "KNeighborsClassifier Precission: [], 1 of KFold 5\n",
      "KNeighborsClassifier AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299], 1 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544], 1 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941], 1 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655], 1 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299], 2 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544], 2 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941], 2 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655], 2 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299, 0.9944515895876958], 2 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544, 0.9900085034013606], 2 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941, 0.9746156596353236], 2 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655, 0.9832281334535617], 2 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719, 0.9847138322721971], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299, 0.9944515895876958], 3 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544, 0.9900085034013606], 3 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941, 0.9746156596353236], 3 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655, 0.9832281334535617], 3 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719, 0.9847138322721971], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299, 0.9944515895876958, 0.9946216558072299], 3 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544, 0.9900085034013606, 0.9883503401360544], 3 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941, 0.9746156596353236, 0.9699731903485255], 3 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655, 0.9832281334535617, 0.9808422194108078], 3 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719, 0.9847138322721971, 0.9820298308785863], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299, 0.9944515895876958, 0.9946216558072299], 4 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544, 0.9900085034013606, 0.9883503401360544], 4 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941, 0.9746156596353236, 0.9699731903485255], 4 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655, 0.9832281334535617, 0.9808422194108078], 4 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719, 0.9847138322721971, 0.9820298308785863], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299, 0.9944515895876958, 0.9946216558072299, 0.994908642552694], 4 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544, 0.9900085034013606, 0.9883503401360544, 0.9893707482993197], 4 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941, 0.9746156596353236, 0.9699731903485255, 0.9717605004468275], 4 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655, 0.9832281334535617, 0.9808422194108078, 0.9833604630132031], 4 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719, 0.9847138322721971, 0.9820298308785863, 0.983314001966789], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299, 0.9944515895876958, 0.9946216558072299, 0.994908642552694], 5 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544, 0.9900085034013606, 0.9883503401360544, 0.9893707482993197], 5 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941, 0.9746156596353236, 0.9699731903485255, 0.9717605004468275], 5 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655, 0.9832281334535617, 0.9808422194108078, 0.9833604630132031], 5 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719, 0.9847138322721971, 0.9820298308785863, 0.983314001966789], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "KNeighborsClassifier Acc Train: [0.9942176870748299, 0.9944515895876958, 0.9946216558072299, 0.994908642552694, 0.9939307617903721], 5 of KFold 5\n",
      "KNeighborsClassifier Acc Test: [0.9891586242081544, 0.9900085034013606, 0.9883503401360544, 0.9893707482993197, 0.9894132653061225], 5 of KFold 5\n",
      "KNeighborsClassifier Recall: [0.974798927613941, 0.9746156596353236, 0.9699731903485255, 0.9717605004468275, 0.972117962466488], 5 of KFold 5\n",
      "KNeighborsClassifier Precission: [0.9795258620689655, 0.9832281334535617, 0.9808422194108078, 0.9833604630132031, 0.9831887201735358], 5 of KFold 5\n",
      "KNeighborsClassifier AUC: [0.9842197248802719, 0.9847138322721971, 0.9820298308785863, 0.983314001966789, 0.9834648389738297], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "KNeighborsClassifier Acc Train: 0.9944260673625642\n",
      "KNeighborsClassifier Acc Test: 0.9892602962702023\n",
      "KNeighborsClassifier Recall: 0.9726532481022211\n",
      "KNeighborsClassifier Precission: 0.9820290796240148\n",
      "KNeighborsClassifier AUC: 0.9835484457943349\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GaussianNB Acc Train: [], 1 of KFold 5\n",
      "GaussianNB Acc Test: [], 1 of KFold 5\n",
      "GaussianNB Recall: [], 1 of KFold 5\n",
      "GaussianNB Precission: [], 1 of KFold 5\n",
      "GaussianNB AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GaussianNB Acc Train: [0.9679528061224489], 1 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003], 1 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048], 1 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562], 1 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GaussianNB Acc Train: [0.9679528061224489], 2 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003], 2 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048], 2 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562], 2 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GaussianNB Acc Train: [0.9679528061224489, 0.96701778254908], 2 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003, 0.9685799319727891], 2 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048, 0.8956024311762603], 2 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562, 0.9699903194578896], 2 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479, 0.943477886345689], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GaussianNB Acc Train: [0.9679528061224489, 0.96701778254908], 3 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003, 0.9685799319727891], 3 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048, 0.8956024311762603], 3 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562, 0.9699903194578896], 3 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479, 0.943477886345689], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GaussianNB Acc Train: [0.9679528061224489, 0.96701778254908, 0.9668477163295458], 3 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003, 0.9685799319727891, 0.9664965986394558], 3 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048, 0.8956024311762603, 0.8895442359249329], 3 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562, 0.9699903194578896, 0.9669710510977269], 3 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479, 0.943477886345689, 0.9400301374882684], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GaussianNB Acc Train: [0.9679528061224489, 0.96701778254908, 0.9668477163295458], 4 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003, 0.9685799319727891, 0.9664965986394558], 4 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048, 0.8956024311762603, 0.8895442359249329], 4 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562, 0.9699903194578896, 0.9669710510977269], 4 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479, 0.943477886345689, 0.9400301374882684], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GaussianNB Acc Train: [0.9679528061224489, 0.96701778254908, 0.9668477163295458, 0.9681019546986108], 4 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003, 0.9685799319727891, 0.9664965986394558, 0.9679421768707483], 4 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048, 0.8956024311762603, 0.8895442359249329, 0.8916890080428954], 4 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562, 0.9699903194578896, 0.9669710510977269, 0.9711894101615729], 4 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479, 0.943477886345689, 0.9400301374882684, 0.9417161916086164], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GaussianNB Acc Train: [0.9679528061224489, 0.96701778254908, 0.9668477163295458, 0.9681019546986108], 5 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003, 0.9685799319727891, 0.9664965986394558, 0.9679421768707483], 5 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048, 0.8956024311762603, 0.8895442359249329, 0.8916890080428954], 5 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562, 0.9699903194578896, 0.9669710510977269, 0.9711894101615729], 5 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479, 0.943477886345689, 0.9400301374882684, 0.9417161916086164], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GaussianNB Acc Train: [0.9679528061224489, 0.96701778254908, 0.9668477163295458, 0.9681019546986108, 0.9688247361316312], 5 of KFold 5\n",
      "GaussianNB Acc Test: [0.9679010246163003, 0.9685799319727891, 0.9664965986394558, 0.9679421768707483, 0.9683673469387755], 5 of KFold 5\n",
      "GaussianNB Recall: [0.8911528150134048, 0.8956024311762603, 0.8895442359249329, 0.8916890080428954, 0.8941912421805184], 5 of KFold 5\n",
      "GaussianNB Precission: [0.9715510522213562, 0.9699903194578896, 0.9669710510977269, 0.9711894101615729, 0.9705140640155189], 5 of KFold 5\n",
      "GaussianNB AUC: [0.94150411028479, 0.943477886345689, 0.9400301374882684, 0.9417161916086164, 0.9428557326662704], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "GaussianNB Acc Train: 0.9677489991662632\n",
      "GaussianNB Acc Test: 0.9678574158076139\n",
      "GaussianNB Recall: 0.8924359464676023\n",
      "GaussianNB Precission: 0.9700431793908129\n",
      "GaussianNB AUC: 0.9419168116787269\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "SVC Acc Train: [], 1 of KFold 5\n",
      "SVC Acc Test: [], 1 of KFold 5\n",
      "SVC Recall: [], 1 of KFold 5\n",
      "SVC Precission: [], 1 of KFold 5\n",
      "SVC AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "SVC Acc Train: [0.9984800170068027], 1 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309], 1 of KFold 5\n",
      "SVC Recall: [0.9912421805183199], 1 of KFold 5\n",
      "SVC Precission: [0.9996395097332372], 1 of KFold 5\n",
      "SVC AUC: [0.9955653053657092], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "SVC Acc Train: [0.9984800170068027], 2 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309], 2 of KFold 5\n",
      "SVC Recall: [0.9912421805183199], 2 of KFold 5\n",
      "SVC Precission: [0.9996395097332372], 2 of KFold 5\n",
      "SVC AUC: [0.9955653053657092], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "SVC Acc Train: [0.9984800170068027, 0.9982036755561697], 2 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309, 0.9985969387755103], 2 of KFold 5\n",
      "SVC Recall: [0.9912421805183199, 0.9946371111905613], 2 of KFold 5\n",
      "SVC Precission: [0.9996395097332372, 0.9994611101131669], 2 of KFold 5\n",
      "SVC AUC: [0.9955653053657092, 0.9972348782551043], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "SVC Acc Train: [0.9984800170068027, 0.9982036755561697], 3 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309, 0.9985969387755103], 3 of KFold 5\n",
      "SVC Recall: [0.9912421805183199, 0.9946371111905613], 3 of KFold 5\n",
      "SVC Precission: [0.9996395097332372, 0.9994611101131669], 3 of KFold 5\n",
      "SVC AUC: [0.9955653053657092, 0.9972348782551043], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "SVC Acc Train: [0.9984800170068027, 0.9982036755561697, 0.9981292715851234], 3 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309, 0.9985969387755103, 0.9980442176870749], 3 of KFold 5\n",
      "SVC Recall: [0.9912421805183199, 0.9946371111905613, 0.9930294906166219], 3 of KFold 5\n",
      "SVC Precission: [0.9996395097332372, 0.9994611101131669, 0.9987416861405717], 3 of KFold 5\n",
      "SVC AUC: [0.9955653053657092, 0.9972348782551043, 0.9963194872887852], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "SVC Acc Train: [0.9984800170068027, 0.9982036755561697, 0.9981292715851234], 4 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309, 0.9985969387755103, 0.9980442176870749], 4 of KFold 5\n",
      "SVC Recall: [0.9912421805183199, 0.9946371111905613, 0.9930294906166219], 4 of KFold 5\n",
      "SVC Precission: [0.9996395097332372, 0.9994611101131669, 0.9987416861405717], 4 of KFold 5\n",
      "SVC AUC: [0.9955653053657092, 0.9972348782551043, 0.9963194872887852], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "SVC Acc Train: [0.9984800170068027, 0.9982036755561697, 0.9981292715851234, 0.9981717881400071], 4 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309, 0.9985969387755103, 0.9980442176870749, 0.9982142857142857], 4 of KFold 5\n",
      "SVC Recall: [0.9912421805183199, 0.9946371111905613, 0.9930294906166219, 0.9930294906166219], 4 of KFold 5\n",
      "SVC Precission: [0.9996395097332372, 0.9994611101131669, 0.9987416861405717, 0.9994603345925526], 4 of KFold 5\n",
      "SVC AUC: [0.9955653053657092, 0.9972348782551043, 0.9963194872887852, 0.9964310632999427], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "SVC Acc Train: [0.9984800170068027, 0.9982036755561697, 0.9981292715851234, 0.9981717881400071], 5 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309, 0.9985969387755103, 0.9980442176870749, 0.9982142857142857], 5 of KFold 5\n",
      "SVC Recall: [0.9912421805183199, 0.9946371111905613, 0.9930294906166219, 0.9930294906166219], 5 of KFold 5\n",
      "SVC Precission: [0.9996395097332372, 0.9994611101131669, 0.9987416861405717, 0.9994603345925526], 5 of KFold 5\n",
      "SVC AUC: [0.9955653053657092, 0.9972348782551043, 0.9963194872887852, 0.9964310632999427], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "SVC Acc Train: [0.9984800170068027, 0.9982036755561697, 0.9981292715851234, 0.9981717881400071, 0.9979166888107057], 5 of KFold 5\n",
      "SVC Acc Test: [0.9978317248416309, 0.9985969387755103, 0.9980442176870749, 0.9982142857142857, 0.9979591836734694], 5 of KFold 5\n",
      "SVC Recall: [0.9912421805183199, 0.9946371111905613, 0.9930294906166219, 0.9930294906166219, 0.9924932975871313], 5 of KFold 5\n",
      "SVC Precission: [0.9996395097332372, 0.9994611101131669, 0.9987416861405717, 0.9994603345925526, 0.9989206691851052], 5 of KFold 5\n",
      "SVC AUC: [0.9955653053657092, 0.9972348782551043, 0.9963194872887852, 0.9964310632999427, 0.9960792847768294], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "SVC Acc Train: 0.9981802882197617\n",
      "SVC Acc Test: 0.9981292701383941\n",
      "SVC Recall: 0.9928863141058513\n",
      "SVC Precission: 0.9992446619529268\n",
      "SVC AUC: 0.9963260037972741\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LinearSVC Acc Train: [], 1 of KFold 5\n",
      "LinearSVC Acc Test: [], 1 of KFold 5\n",
      "LinearSVC Recall: [], 1 of KFold 5\n",
      "LinearSVC Precission: [], 1 of KFold 5\n",
      "LinearSVC AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LinearSVC Acc Train: [0.9636904761904762], 1 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154], 1 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687], 1 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944], 1 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LinearSVC Acc Train: [0.9636904761904762], 2 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154], 2 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687], 2 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944], 2 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LinearSVC Acc Train: [0.9636904761904762, 0.9125647048819634], 2 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154, 0.9139030612244898], 2 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687, 0.9742581337146943], 2 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944, 0.743418360387396], 2 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365, 0.9346633745668195], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LinearSVC Acc Train: [0.9636904761904762, 0.9125647048819634], 3 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154, 0.9139030612244898], 3 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687, 0.9742581337146943], 3 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944, 0.743418360387396], 3 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365, 0.9346633745668195], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LinearSVC Acc Train: [0.9636904761904762, 0.9125647048819634, 0.9510102996354205], 3 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154, 0.9139030612244898, 0.9531462585034014], 3 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687, 0.9742581337146943, 0.8296693476318141], 3 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944, 0.743418360387396, 0.9689000208724692], 3 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365, 0.9346633745668195, 0.9106784674002864], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LinearSVC Acc Train: [0.9636904761904762, 0.9125647048819634, 0.9510102996354205], 4 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154, 0.9139030612244898, 0.9531462585034014], 4 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687, 0.9742581337146943, 0.8296693476318141], 4 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944, 0.743418360387396, 0.9689000208724692], 4 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365, 0.9346633745668195, 0.9106784674002864], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LinearSVC Acc Train: [0.9636904761904762, 0.9125647048819634, 0.9510102996354205, 0.7568584517596539], 4 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154, 0.9139030612244898, 0.9531462585034014, 0.7559523809523809], 4 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687, 0.9742581337146943, 0.8296693476318141, 0.9892761394101877], 4 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944, 0.743418360387396, 0.9689000208724692, 0.4935354436023183], 4 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365, 0.9346633745668195, 0.9106784674002864, 0.8362001338613002], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "LinearSVC Acc Train: [0.9636904761904762, 0.9125647048819634, 0.9510102996354205, 0.7568584517596539], 5 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154, 0.9139030612244898, 0.9531462585034014, 0.7559523809523809], 5 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687, 0.9742581337146943, 0.8296693476318141, 0.9892761394101877], 5 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944, 0.743418360387396, 0.9689000208724692, 0.4935354436023183], 5 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365, 0.9346633745668195, 0.9106784674002864, 0.8362001338613002], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "LinearSVC Acc Train: [0.9636904761904762, 0.9125647048819634, 0.9510102996354205, 0.7568584517596539, 0.9597687099414335], 5 of KFold 5\n",
      "LinearSVC Acc Test: [0.9639471110922154, 0.9139030612244898, 0.9531462585034014, 0.7559523809523809, 0.9583333333333334], 5 of KFold 5\n",
      "LinearSVC Recall: [0.8684539767649687, 0.9742581337146943, 0.8296693476318141, 0.9892761394101877, 0.9712243074173369], 5 of KFold 5\n",
      "LinearSVC Precission: [0.9774693220679944, 0.743418360387396, 0.9689000208724692, 0.4935354436023183, 0.8690228690228691], 5 of KFold 5\n",
      "LinearSVC AUC: [0.9311030343492365, 0.9346633745668195, 0.9106784674002864, 0.8362001338613002, 0.9627669654241496], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "LinearSVC Acc Train: 0.9087785284817895\n",
      "LinearSVC Acc Test: 0.9090564290211642\n",
      "LinearSVC Recall: 0.9265763809878003\n",
      "LinearSVC Precission: 0.8104692031906093\n",
      "LinearSVC AUC: 0.9150823951203584\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "RandomForestClassifier Acc Train: [], 1 of KFold 5\n",
      "RandomForestClassifier Acc Test: [], 1 of KFold 5\n",
      "RandomForestClassifier Recall: [], 1 of KFold 5\n",
      "RandomForestClassifier Precission: [], 1 of KFold 5\n",
      "RandomForestClassifier AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "RandomForestClassifier Acc Train: [1.0], 1 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0], 1 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0], 1 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0], 1 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "RandomForestClassifier Acc Train: [1.0], 2 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0], 2 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0], 2 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0], 2 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "RandomForestClassifier Acc Train: [1.0, 1.0], 2 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0, 1.0], 2 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0, 1.0], 2 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0, 1.0], 2 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0, 1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "RandomForestClassifier Acc Train: [1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "RandomForestClassifier Acc Train: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "RandomForestClassifier Acc Train: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "RandomForestClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "RandomForestClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "RandomForestClassifier Acc Train: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier Acc Test: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier Recall: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier Precission: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "RandomForestClassifier AUC: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "RandomForestClassifier Acc Train: 1.0\n",
      "RandomForestClassifier Acc Test: 1.0\n",
      "RandomForestClassifier Recall: 1.0\n",
      "RandomForestClassifier Precission: 1.0\n",
      "RandomForestClassifier AUC: 1.0\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GradientBoostingClassifier Acc Train: [], 1 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [], 1 of KFold 5\n",
      "GradientBoostingClassifier Recall: [], 1 of KFold 5\n",
      "GradientBoostingClassifier Precission: [], 1 of KFold 5\n",
      "GradientBoostingClassifier AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GradientBoostingClassifier Acc Train: [1.0], 1 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0], 1 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0], 1 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0], 1 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GradientBoostingClassifier Acc Train: [1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GradientBoostingClassifier Acc Train: [1.0, 1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0, 1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0, 1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0, 1.0], 2 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0, 1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GradientBoostingClassifier Acc Train: [1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GradientBoostingClassifier Acc Train: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GradientBoostingClassifier Acc Train: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GradientBoostingClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "GradientBoostingClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "GradientBoostingClassifier Acc Train: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier Acc Test: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier Recall: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier Precission: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "GradientBoostingClassifier AUC: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "GradientBoostingClassifier Acc Train: 1.0\n",
      "GradientBoostingClassifier Acc Test: 1.0\n",
      "GradientBoostingClassifier Recall: 1.0\n",
      "GradientBoostingClassifier Precission: 1.0\n",
      "GradientBoostingClassifier AUC: 1.0\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "ExtraTreesClassifier Acc Train: [], 1 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [], 1 of KFold 5\n",
      "ExtraTreesClassifier Recall: [], 1 of KFold 5\n",
      "ExtraTreesClassifier Precission: [], 1 of KFold 5\n",
      "ExtraTreesClassifier AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "ExtraTreesClassifier Acc Train: [1.0], 1 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651], 1 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792], 1 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0], 1 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "ExtraTreesClassifier Acc Train: [1.0], 2 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651], 2 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792], 2 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0], 2 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "ExtraTreesClassifier Acc Train: [1.0, 1.0], 2 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651, 0.9998299319727891], 2 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792, 0.9998212370396854], 2 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0, 0.9994639027877055], 2 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395, 0.9998269411796665], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "ExtraTreesClassifier Acc Train: [1.0, 1.0], 3 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651, 0.9998299319727891], 3 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792, 0.9998212370396854], 3 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0, 0.9994639027877055], 3 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395, 0.9998269411796665], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "ExtraTreesClassifier Acc Train: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651, 0.9998299319727891, 0.9997448979591836], 3 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792, 0.9998212370396854, 0.9994638069705094], 3 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0, 0.9994639027877055, 0.9994638069705094], 3 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395, 0.9998269411796665, 0.9996482214768866], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "ExtraTreesClassifier Acc Train: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651, 0.9998299319727891, 0.9997448979591836], 4 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792, 0.9998212370396854, 0.9994638069705094], 4 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0, 0.9994639027877055, 0.9994638069705094], 4 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395, 0.9998269411796665, 0.9996482214768866], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "ExtraTreesClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651, 0.9998299319727891, 0.9997448979591836, 0.9997874149659864], 4 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792, 0.9998212370396854, 0.9994638069705094, 0.9994638069705094], 4 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0, 0.9994639027877055, 0.9994638069705094, 0.9996424740793708], 4 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395, 0.9998269411796665, 0.9996482214768866, 0.9996761154796759], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "ExtraTreesClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651, 0.9998299319727891, 0.9997448979591836, 0.9997874149659864], 5 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792, 0.9998212370396854, 0.9994638069705094, 0.9994638069705094], 5 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0, 0.9994639027877055, 0.9994638069705094, 0.9996424740793708], 5 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395, 0.9998269411796665, 0.9996482214768866, 0.9996761154796759], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "ExtraTreesClassifier Acc Train: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "ExtraTreesClassifier Acc Test: [0.9998299392032651, 0.9998299319727891, 0.9997448979591836, 0.9997874149659864, 0.9998724489795918], 5 of KFold 5\n",
      "ExtraTreesClassifier Recall: [0.9992850759606792, 0.9998212370396854, 0.9994638069705094, 0.9994638069705094, 0.9994638069705094], 5 of KFold 5\n",
      "ExtraTreesClassifier Precission: [1.0, 0.9994639027877055, 0.9994638069705094, 0.9996424740793708, 1.0], 5 of KFold 5\n",
      "ExtraTreesClassifier AUC: [0.9996425379803395, 0.9998269411796665, 0.9996482214768866, 0.9996761154796759, 0.9997319034852548], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "ExtraTreesClassifier Acc Train: 1.0\n",
      "ExtraTreesClassifier Acc Test: 0.9998129266161632\n",
      "ExtraTreesClassifier Recall: 0.9994995467823786\n",
      "ExtraTreesClassifier Precission: 0.9997140367675172\n",
      "ExtraTreesClassifier AUC: 0.9997051439203647\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "XGBClassifier Acc Train: [], 1 of KFold 5\n",
      "XGBClassifier Acc Test: [], 1 of KFold 5\n",
      "XGBClassifier Recall: [], 1 of KFold 5\n",
      "XGBClassifier Precission: [], 1 of KFold 5\n",
      "XGBClassifier AUC: [], 1 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "XGBClassifier Acc Train: [1.0], 1 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0], 1 of KFold 5\n",
      "XGBClassifier Recall: [1.0], 1 of KFold 5\n",
      "XGBClassifier Precission: [1.0], 1 of KFold 5\n",
      "XGBClassifier AUC: [1.0], 1 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "XGBClassifier Acc Train: [1.0], 2 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0], 2 of KFold 5\n",
      "XGBClassifier Recall: [1.0], 2 of KFold 5\n",
      "XGBClassifier Precission: [1.0], 2 of KFold 5\n",
      "XGBClassifier AUC: [1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "XGBClassifier Acc Train: [1.0, 1.0], 2 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0, 1.0], 2 of KFold 5\n",
      "XGBClassifier Recall: [1.0, 1.0], 2 of KFold 5\n",
      "XGBClassifier Precission: [1.0, 1.0], 2 of KFold 5\n",
      "XGBClassifier AUC: [1.0, 1.0], 2 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "XGBClassifier Acc Train: [1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier Recall: [1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier Precission: [1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier AUC: [1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "XGBClassifier Acc Train: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier Recall: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier Precission: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "XGBClassifier AUC: [1.0, 1.0, 1.0], 3 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "XGBClassifier Acc Train: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier Recall: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier Precission: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier AUC: [1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "XGBClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier Recall: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier Precission: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "XGBClassifier AUC: [1.0, 1.0, 1.0, 1.0], 4 of KFold 5\n",
      "---------------------------\n",
      "----------BEFORE------------\n",
      "XGBClassifier Acc Train: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier Recall: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier Precission: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier AUC: [1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------AFTER------------\n",
      "XGBClassifier Acc Train: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier Acc Test: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier Recall: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier Precission: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "XGBClassifier AUC: [1.0, 1.0, 1.0, 1.0, 1.0], 5 of KFold 5\n",
      "---------------------------\n",
      "----------FINAL------------\n",
      "XGBClassifier Acc Train: 1.0\n",
      "XGBClassifier Acc Test: 1.0\n",
      "XGBClassifier Recall: 1.0\n",
      "XGBClassifier Precission: 1.0\n",
      "XGBClassifier AUC: 1.0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Modelling Algorithms\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "## Collect all model in one list\n",
    "all_model = [DecisionTreeClassifier,\n",
    "            LogisticRegression,\n",
    "             KNeighborsClassifier,\n",
    "             GaussianNB,\n",
    "            SVC,\n",
    "            LinearSVC,\n",
    "            RandomForestClassifier,\n",
    "            GradientBoostingClassifier,\n",
    "            ExtraTreesClassifier,\n",
    "             XGBClassifier]\n",
    "\n",
    "model_name = ['DecisionTreeClassifier',\n",
    "            'LogisticRegression',\n",
    "             'KNeighborsClassifier',\n",
    "             'GaussianNB',\n",
    "            'SVC',\n",
    "            'LinearSVC',\n",
    "            'RandomForestClassifier',\n",
    "            'GradientBoostingClassifier',\n",
    "            'ExtraTreesClassifier',\n",
    "             'XGBClassifier']\n",
    "## loop for all model\n",
    "\n",
    "datatr = []\n",
    "datasc = []\n",
    "Recall =[]\n",
    "Precision =[]\n",
    "auc =[]\n",
    "\n",
    "for idx, model_type in enumerate(all_model):\n",
    "    num = 1\n",
    "    AccTrain = []\n",
    "    AccTest = []\n",
    "    RecallTemp = []\n",
    "    PrecisionTemp = []\n",
    "    AucTemp = []\n",
    "    nfold = 1\n",
    "    for train_index,test_index in kf.split(X,y): \n",
    "\n",
    "        print(\"----------BEFORE------------\")\n",
    "        print(\"{} Acc Train: {}, {} of KFold {}\".format(model_name[idx], AccTrain, nfold, kf.n_splits))\n",
    "        print(\"{} Acc Test: {}, {} of KFold {}\".format(model_name[idx], AccTest, nfold, kf.n_splits))\n",
    "        print(\"{} Recall: {}, {} of KFold {}\".format(model_name[idx], RecallTemp, nfold, kf.n_splits))\n",
    "        print(\"{} Precission: {}, {} of KFold {}\".format(model_name[idx], PrecisionTemp, nfold, kf.n_splits))\n",
    "        print(\"{} AUC: {}, {} of KFold {}\".format(model_name[idx], AucTemp, nfold, kf.n_splits))\n",
    "        print(\"---------------------------\")\n",
    "        \n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        \n",
    "        model = model_type()\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred=model.predict(X_test)\n",
    "        \n",
    "        AccTrain.append(model.score(X_train , y_train))\n",
    "        AccTest.append(model.score(X_test , y_test))\n",
    "        RecallTemp.append(recall_score(y_test,y_pred))\n",
    "        PrecisionTemp.append(precision_score(y_test,y_pred))\n",
    "        AucTemp.append(roc_auc_score(y_test, y_pred))\n",
    "        \n",
    "        print(\"----------AFTER------------\")\n",
    "        print(\"{} Acc Train: {}, {} of KFold {}\".format(model_name[idx], AccTrain, nfold, kf.n_splits))\n",
    "        print(\"{} Acc Test: {}, {} of KFold {}\".format(model_name[idx], AccTest, nfold, kf.n_splits))\n",
    "        print(\"{} Recall: {}, {} of KFold {}\".format(model_name[idx], RecallTemp, nfold, kf.n_splits))\n",
    "        print(\"{} Precission: {}, {} of KFold {}\".format(model_name[idx], PrecisionTemp, nfold, kf.n_splits))\n",
    "        print(\"{} AUC: {}, {} of KFold {}\".format(model_name[idx], AucTemp, nfold, kf.n_splits))\n",
    "        print(\"---------------------------\")\n",
    "        \n",
    "        nfold += 1\n",
    "    \n",
    "    print(\"----------FINAL------------\")\n",
    "    print(\"{} Acc Train: {}\".format(model_name[idx], np.mean(AccTrain)))\n",
    "    print(\"{} Acc Test: {}\".format(model_name[idx], np.mean(AccTest)))\n",
    "    print(\"{} Recall: {}\".format(model_name[idx], np.mean(RecallTemp)))\n",
    "    print(\"{} Precission: {}\".format(model_name[idx], np.mean(PrecisionTemp)))\n",
    "    print(\"{} AUC: {}\".format(model_name[idx], np.mean(AucTemp)))\n",
    "    print(\"---------------------------\")\n",
    "    datatr.append(np.mean(AccTrain))\n",
    "    datasc.append(np.mean(AccTest))\n",
    "    Recall.append(np.mean(RecallTemp))\n",
    "    Precision.append(np.mean(PrecisionTemp))\n",
    "    auc.append(np.mean(AucTemp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c112590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:26:53.133538Z",
     "iopub.status.busy": "2022-11-02T13:26:53.133129Z",
     "iopub.status.idle": "2022-11-02T13:26:53.157736Z",
     "shell.execute_reply": "2022-11-02T13:26:53.156891Z"
    },
    "papermill": {
     "duration": 0.054417,
     "end_time": "2022-11-02T13:26:53.159796",
     "exception": false,
     "start_time": "2022-11-02T13:26:53.105379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy training</th>\n",
       "      <th>Accuracy test</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999813</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.998180</td>\n",
       "      <td>0.998129</td>\n",
       "      <td>0.999245</td>\n",
       "      <td>0.992886</td>\n",
       "      <td>0.996326</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.994426</td>\n",
       "      <td>0.989260</td>\n",
       "      <td>0.982029</td>\n",
       "      <td>0.972653</td>\n",
       "      <td>0.983548</td>\n",
       "      <td>0.005166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.967749</td>\n",
       "      <td>0.967857</td>\n",
       "      <td>0.970043</td>\n",
       "      <td>0.892436</td>\n",
       "      <td>0.941917</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.957860</td>\n",
       "      <td>0.957960</td>\n",
       "      <td>0.902356</td>\n",
       "      <td>0.924786</td>\n",
       "      <td>0.946550</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.908779</td>\n",
       "      <td>0.909056</td>\n",
       "      <td>0.810469</td>\n",
       "      <td>0.926576</td>\n",
       "      <td>0.915082</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  Accuracy training  Accuracy test  Precision  \\\n",
       "0      DecisionTreeClassifier           1.000000       1.000000   1.000000   \n",
       "6      RandomForestClassifier           1.000000       1.000000   1.000000   \n",
       "7  GradientBoostingClassifier           1.000000       1.000000   1.000000   \n",
       "9               XGBClassifier           1.000000       1.000000   1.000000   \n",
       "8        ExtraTreesClassifier           1.000000       0.999813   0.999714   \n",
       "4                         SVC           0.998180       0.998129   0.999245   \n",
       "2        KNeighborsClassifier           0.994426       0.989260   0.982029   \n",
       "3                  GaussianNB           0.967749       0.967857   0.970043   \n",
       "1          LogisticRegression           0.957860       0.957960   0.902356   \n",
       "5                   LinearSVC           0.908779       0.909056   0.810469   \n",
       "\n",
       "     Recall       AUC       gap  \n",
       "0  1.000000  1.000000  0.000000  \n",
       "6  1.000000  1.000000  0.000000  \n",
       "7  1.000000  1.000000  0.000000  \n",
       "9  1.000000  1.000000  0.000000  \n",
       "8  0.999500  0.999705  0.000187  \n",
       "4  0.992886  0.996326  0.000051  \n",
       "2  0.972653  0.983548  0.005166  \n",
       "3  0.892436  0.941917  0.000108  \n",
       "1  0.924786  0.946550  0.000100  \n",
       "5  0.926576  0.915082  0.000278  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compare model each other\n",
    "data_hasil = pd.DataFrame()\n",
    "data_hasil['model'] = model_name\n",
    "data_hasil['Accuracy training'] = datatr\n",
    "data_hasil['Accuracy test'] = datasc\n",
    "data_hasil['Precision'] = Precision\n",
    "data_hasil['Recall']= Recall\n",
    "data_hasil['AUC']=auc\n",
    "data_hasil['gap'] = abs(data_hasil['Accuracy training'] - data_hasil['Accuracy test'])\n",
    "data_hasil.sort_values(by='Accuracy test',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ced017",
   "metadata": {
    "papermill": {
     "duration": 0.025044,
     "end_time": "2022-11-02T13:26:53.210942",
     "exception": false,
     "start_time": "2022-11-02T13:26:53.185898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparamater Tunning, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "669935b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:26:53.263598Z",
     "iopub.status.busy": "2022-11-02T13:26:53.262754Z",
     "iopub.status.idle": "2022-11-02T13:26:53.267499Z",
     "shell.execute_reply": "2022-11-02T13:26:53.266701Z"
    },
    "papermill": {
     "duration": 0.033243,
     "end_time": "2022-11-02T13:26:53.269491",
     "exception": false,
     "start_time": "2022-11-02T13:26:53.236248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f7e29bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:26:53.322267Z",
     "iopub.status.busy": "2022-11-02T13:26:53.321300Z",
     "iopub.status.idle": "2022-11-02T13:26:53.325563Z",
     "shell.execute_reply": "2022-11-02T13:26:53.324856Z"
    },
    "papermill": {
     "duration": 0.032954,
     "end_time": "2022-11-02T13:26:53.327742",
     "exception": false,
     "start_time": "2022-11-02T13:26:53.294788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad7b2119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:26:53.380740Z",
     "iopub.status.busy": "2022-11-02T13:26:53.380072Z",
     "iopub.status.idle": "2022-11-02T13:43:16.932248Z",
     "shell.execute_reply": "2022-11-02T13:43:16.931099Z"
    },
    "papermill": {
     "duration": 983.581366,
     "end_time": "2022-11-02T13:43:16.934789",
     "exception": false,
     "start_time": "2022-11-02T13:26:53.353423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=1e-05, penalty=none, solver=newton-cg;, score=1.000 total time=   6.7s\n",
      "[CV 3/5] END C=1e-05, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=1e-05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1e-05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1e-05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1e-05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1e-05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1e-05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1e-05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1e-05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1e-05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1e-05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=1e-05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=1e-05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=1e-05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=1e-05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=1e-05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1e-05, penalty=l1, solver=liblinear;, score=0.791 total time=   0.3s\n",
      "[CV 2/5] END C=1e-05, penalty=l1, solver=liblinear;, score=0.788 total time=   0.3s\n",
      "[CV 5/5] END C=1e-05, penalty=l1, solver=liblinear;, score=0.789 total time=   0.4s\n",
      "[CV 3/5] END C=1e-05, penalty=l2, solver=newton-cg;, score=1.000 total time=   6.9s\n",
      "[CV 1/5] END .C=1e-05, penalty=l2, solver=lbfgs;, score=1.000 total time=   1.1s\n",
      "[CV 3/5] END .C=1e-05, penalty=l2, solver=lbfgs;, score=1.000 total time=   1.1s\n",
      "[CV 5/5] END .C=1e-05, penalty=l2, solver=lbfgs;, score=1.000 total time=   1.2s\n",
      "[CV 3/5] END C=1e-05, penalty=l2, solver=liblinear;, score=0.805 total time=   0.6s\n",
      "[CV 1/5] END C=1e-05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1e-05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1e-05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1e-05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1e-05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1e-05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1e-05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1e-05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1e-05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1e-05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1e-05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.0001, penalty=none, solver=newton-cg;, score=0.998 total time=   6.4s\n",
      "[CV 5/5] END C=0.0001, penalty=none, solver=newton-cg;, score=0.998 total time=   6.7s\n",
      "[CV 4/5] END C=0.0001, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.0s\n",
      "[CV 2/5] END C=0.0001, penalty=l2, solver=liblinear;, score=0.848 total time=   0.7s\n",
      "[CV 4/5] END C=0.0001, penalty=l2, solver=liblinear;, score=0.844 total time=   0.8s\n",
      "[CV 2/5] END C=0.001, penalty=none, solver=newton-cg;, score=1.000 total time=   7.0s\n",
      "[CV 1/5] END C=0.001, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 3/5] END C=0.001, penalty=none, solver=lbfgs;, score=0.973 total time=   1.2s\n",
      "[CV 1/5] END C=0.001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.981 total time=   1.3s\n",
      "[CV 1/5] END C=0.001, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.7s\n",
      "[CV 5/5] END C=0.001, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 4/5] END C=0.003, penalty=none, solver=newton-cg;, score=0.995 total time=   6.5s\n",
      "[CV 3/5] END C=0.003, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.003, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.003, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.003, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.003, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.003, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.003, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.003, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.003, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.003, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.003, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=0.003, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=0.003, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=0.003, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=0.003, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=0.003, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.003, penalty=l1, solver=liblinear;, score=0.994 total time=   2.5s\n",
      "[CV 4/5] END C=0.003, penalty=l1, solver=liblinear;, score=0.991 total time=   3.0s\n",
      "[CV 3/5] END C=0.003, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.4s\n",
      "[CV 3/5] END .C=0.003, penalty=l2, solver=lbfgs;, score=0.992 total time=   1.2s\n",
      "[CV 5/5] END .C=0.003, penalty=l2, solver=lbfgs;, score=0.925 total time=   1.2s\n",
      "[CV 3/5] END C=0.003, penalty=l2, solver=liblinear;, score=0.958 total time=   1.7s\n",
      "[CV 1/5] END C=0.003, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.003, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.003, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.003, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.003, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.003, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.003, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.003, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.003, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.003, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.003, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.003, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.003, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.003, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.005, penalty=none, solver=newton-cg;, score=0.998 total time=   6.2s\n",
      "[CV 5/5] END C=0.005, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 5/5] END C=0.005, penalty=l1, solver=liblinear;, score=0.996 total time=   4.3s\n",
      "[CV 4/5] END C=0.005, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.9s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=1e-05, penalty=none, solver=newton-cg;, score=0.998 total time=   6.7s\n",
      "[CV 2/5] END C=1e-05, penalty=none, solver=lbfgs;, score=0.997 total time=   1.2s\n",
      "[CV 5/5] END C=1e-05, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 4/5] END C=1e-05, penalty=l1, solver=liblinear;, score=0.782 total time=   0.4s\n",
      "[CV 2/5] END C=1e-05, penalty=l2, solver=newton-cg;, score=1.000 total time=   7.0s\n",
      "[CV 2/5] END .C=1e-05, penalty=l2, solver=lbfgs;, score=1.000 total time=   1.2s\n",
      "[CV 4/5] END .C=1e-05, penalty=l2, solver=lbfgs;, score=0.944 total time=   1.1s\n",
      "[CV 1/5] END C=1e-05, penalty=l2, solver=liblinear;, score=0.810 total time=   0.6s\n",
      "[CV 2/5] END C=1e-05, penalty=l2, solver=liblinear;, score=0.802 total time=   0.5s\n",
      "[CV 5/5] END C=1e-05, penalty=l2, solver=liblinear;, score=0.805 total time=   0.5s\n",
      "[CV 5/5] END C=1e-05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1e-05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1e-05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1e-05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.0001, penalty=none, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 1/5] END C=0.0001, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 3/5] END C=0.0001, penalty=none, solver=lbfgs;, score=0.973 total time=   1.2s\n",
      "[CV 1/5] END C=0.0001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.0001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.0001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.0001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.0001, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.0001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.0001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.0001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.0001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.0001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ..C=0.0001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ..C=0.0001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ..C=0.0001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ..C=0.0001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.0001, penalty=l1, solver=liblinear;, score=0.792 total time=   0.4s\n",
      "[CV 4/5] END C=0.0001, penalty=l1, solver=liblinear;, score=0.789 total time=   0.4s\n",
      "[CV 2/5] END C=0.0001, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 2/5] END C=0.0001, penalty=l2, solver=lbfgs;, score=0.997 total time=   1.2s\n",
      "[CV 4/5] END C=0.0001, penalty=l2, solver=lbfgs;, score=0.933 total time=   1.2s\n",
      "[CV 1/5] END C=0.0001, penalty=l2, solver=liblinear;, score=0.855 total time=   0.6s\n",
      "[CV 3/5] END C=0.0001, penalty=l2, solver=liblinear;, score=0.846 total time=   0.7s\n",
      "[CV 1/5] END C=0.0001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.0001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.0001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.0001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.0001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.0001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.0001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.0001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.0001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.0001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.0001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.0001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.0001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.0001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.0001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, penalty=none, solver=newton-cg;, score=0.998 total time=   6.9s\n",
      "[CV 5/5] END C=0.001, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 4/5] END C=0.001, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.2s\n",
      "[CV 3/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.992 total time=   1.2s\n",
      "[CV 1/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.935 total time=   1.3s\n",
      "[CV 4/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.924 total time=   1.3s\n",
      "[CV 2/5] END C=0.003, penalty=none, solver=newton-cg;, score=1.000 total time=   7.0s\n",
      "[CV 1/5] END C=0.003, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 4/5] END C=0.003, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 3/5] END C=0.003, penalty=l1, solver=liblinear;, score=0.994 total time=   3.3s\n",
      "[CV 2/5] END C=0.003, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 5/5] END C=0.003, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.8s\n",
      "[CV 3/5] END C=0.005, penalty=none, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 2/5] END C=0.005, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 5/5] END C=0.005, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 3/5] END C=0.005, penalty=l1, solver=liblinear;, score=0.996 total time=   3.5s\n",
      "[CV 1/5] END C=0.005, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.3s\n",
      "[CV 5/5] END C=0.005, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 4/5] END C=0.005, penalty=l2, solver=liblinear;, score=0.953 total time=   1.4s\n",
      "[CV 1/5] END C=0.005, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.005, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.005, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.005, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.005, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.005, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.005, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.005, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.005, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.005, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.005, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.005, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.005, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.005, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.005, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, penalty=none, solver=newton-cg;, score=1.000 total time=   6.8s\n",
      "[CV 1/5] END C=0.01, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 3/5] END C=0.01, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.01, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.01, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.01, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.01, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.01, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s[CV 4/5] END C=1e-05, penalty=none, solver=newton-cg;, score=0.995 total time=   6.2s\n",
      "[CV 5/5] END C=1e-05, penalty=none, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 4/5] END C=1e-05, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.9s\n",
      "[CV 4/5] END C=1e-05, penalty=l2, solver=liblinear;, score=0.801 total time=   0.6s\n",
      "[CV 2/5] END C=0.0001, penalty=none, solver=newton-cg;, score=1.000 total time=   6.7s\n",
      "[CV 2/5] END C=0.0001, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 4/5] END C=0.0001, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 4/5] END ..C=0.0001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.0001, penalty=l1, solver=liblinear;, score=0.798 total time=   0.4s\n",
      "[CV 3/5] END C=0.0001, penalty=l1, solver=liblinear;, score=0.793 total time=   0.3s\n",
      "[CV 1/5] END C=0.0001, penalty=l2, solver=newton-cg;, score=1.000 total time=   6.7s\n",
      "[CV 1/5] END C=0.0001, penalty=l2, solver=lbfgs;, score=0.998 total time=   1.1s\n",
      "[CV 3/5] END C=0.0001, penalty=l2, solver=lbfgs;, score=0.905 total time=   1.1s\n",
      "[CV 5/5] END C=0.0001, penalty=l2, solver=lbfgs;, score=0.919 total time=   1.1s\n",
      "[CV 5/5] END C=0.0001, penalty=l2, solver=liblinear;, score=0.849 total time=   0.7s\n",
      "[CV 3/5] END C=0.001, penalty=none, solver=newton-cg;, score=0.999 total time=   7.1s\n",
      "[CV 2/5] END C=0.001, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 4/5] END C=0.001, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 1/5] END C=0.001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.982 total time=   1.1s\n",
      "[CV 4/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.978 total time=   1.4s\n",
      "[CV 2/5] END C=0.001, penalty=l2, solver=newton-cg;, score=1.000 total time=   6.8s\n",
      "[CV 2/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=1.000 total time=   1.2s\n",
      "[CV 5/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.983 total time=   1.2s\n",
      "[CV 3/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.928 total time=   1.2s\n",
      "[CV 1/5] END C=0.001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.001, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.003, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 5/5] END C=0.003, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 1/5] END C=0.003, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.7s\n",
      "[CV 1/5] END .C=0.003, penalty=l2, solver=lbfgs;, score=0.994 total time=   1.1s\n",
      "[CV 2/5] END .C=0.003, penalty=l2, solver=lbfgs;, score=0.994 total time=   1.1s\n",
      "[CV 4/5] END .C=0.003, penalty=l2, solver=lbfgs;, score=0.954 total time=   1.1s\n",
      "[CV 2/5] END C=0.003, penalty=l2, solver=liblinear;, score=0.955 total time=   1.8s\n",
      "[CV 5/5] END C=0.003, penalty=l2, solver=liblinear;, score=0.960 total time=   1.7s\n",
      "[CV 4/5] END C=0.005, penalty=none, solver=newton-cg;, score=0.995 total time=   6.1s\n",
      "[CV 3/5] END C=0.005, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.005, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.005, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.005, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.005, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.005, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.005, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.005, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.005, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.005, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.005, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=0.005, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=0.005, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=0.005, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=0.005, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=0.005, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.005, penalty=l1, solver=liblinear;, score=0.996 total time=   4.4s\n",
      "[CV 2/5] END C=0.005, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 1/5] END .C=0.005, penalty=l2, solver=lbfgs;, score=0.995 total time=   1.5s\n",
      "[CV 2/5] END .C=0.005, penalty=l2, solver=lbfgs;, score=0.973 total time=   1.3s\n",
      "[CV 4/5] END .C=0.005, penalty=l2, solver=lbfgs;, score=0.960 total time=   1.1s\n",
      "[CV 1/5] END C=0.005, penalty=l2, solver=liblinear;, score=0.973 total time=   4.2s\n",
      "[CV 1/5] END C=0.01, penalty=none, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 5/5] END C=0.01, penalty=none, solver=newton-cg;, score=0.998 total time=   6.7s\n",
      "[CV 4/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.998 total time=   7.3s\n",
      "[CV 3/5] END C=0.01, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.7s\n",
      "[CV 4/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.990 total time=   1.1s\n",
      "[CV 1/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.961 total time=   4.2s\n",
      "[CV 1/5] END C=0.01, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.01, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.01, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.01, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.01, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.01, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.01, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.01, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.01, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.01, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.01, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.01, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.03, penalty=none, solver=newton-cg;, score=0.998 total time=   6.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END C=1e-05, penalty=none, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 1/5] END C=1e-05, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 4/5] END C=1e-05, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 3/5] END C=1e-05, penalty=l1, solver=liblinear;, score=0.788 total time=   0.4s\n",
      "[CV 1/5] END C=1e-05, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 5/5] END C=1e-05, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.7s\n",
      "[CV 4/5] END C=0.0001, penalty=none, solver=newton-cg;, score=0.995 total time=   6.3s\n",
      "[CV 5/5] END C=0.0001, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 5/5] END C=0.0001, penalty=l1, solver=liblinear;, score=0.793 total time=   0.4s\n",
      "[CV 3/5] END C=0.0001, penalty=l2, solver=newton-cg;, score=1.000 total time=   6.2s\n",
      "[CV 5/5] END C=0.0001, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.3s\n",
      "[CV 4/5] END C=0.001, penalty=none, solver=newton-cg;, score=0.995 total time=   6.6s\n",
      "[CV 5/5] END C=0.001, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 3/5] END ...C=0.001, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.982 total time=   1.1s\n",
      "[CV 5/5] END C=0.001, penalty=l1, solver=liblinear;, score=0.981 total time=   1.4s\n",
      "[CV 3/5] END C=0.001, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 1/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.996 total time=   1.2s\n",
      "[CV 4/5] END .C=0.001, penalty=l2, solver=lbfgs;, score=0.954 total time=   1.2s\n",
      "[CV 2/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.929 total time=   1.5s\n",
      "[CV 5/5] END C=0.001, penalty=l2, solver=liblinear;, score=0.927 total time=   1.5s\n",
      "[CV 3/5] END C=0.003, penalty=none, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 2/5] END C=0.003, penalty=none, solver=lbfgs;, score=0.997 total time=   1.2s\n",
      "[CV 5/5] END C=0.003, penalty=none, solver=lbfgs;, score=0.929 total time=   1.1s\n",
      "[CV 2/5] END C=0.003, penalty=l1, solver=liblinear;, score=0.994 total time=   3.1s\n",
      "[CV 5/5] END C=0.003, penalty=l1, solver=liblinear;, score=0.994 total time=   3.0s\n",
      "[CV 4/5] END C=0.003, penalty=l2, solver=newton-cg;, score=0.997 total time=   6.4s\n",
      "[CV 1/5] END C=0.003, penalty=l2, solver=liblinear;, score=0.961 total time=   1.9s\n",
      "[CV 4/5] END C=0.003, penalty=l2, solver=liblinear;, score=0.945 total time=   1.3s\n",
      "[CV 5/5] END C=0.003, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.005, penalty=none, solver=newton-cg;, score=1.000 total time=   6.5s\n",
      "[CV 1/5] END C=0.005, penalty=none, solver=lbfgs;, score=0.975 total time=   1.1s\n",
      "[CV 4/5] END C=0.005, penalty=none, solver=lbfgs;, score=0.991 total time=   1.1s\n",
      "[CV 1/5] END C=0.005, penalty=l1, solver=liblinear;, score=0.996 total time=   4.0s\n",
      "[CV 4/5] END C=0.005, penalty=l1, solver=liblinear;, score=0.993 total time=   3.4s\n",
      "[CV 3/5] END C=0.005, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.2s\n",
      "[CV 3/5] END .C=0.005, penalty=l2, solver=lbfgs;, score=0.998 total time=   1.2s\n",
      "[CV 5/5] END .C=0.005, penalty=l2, solver=lbfgs;, score=0.943 total time=   1.1s\n",
      "[CV 3/5] END C=0.005, penalty=l2, solver=liblinear;, score=0.965 total time=   4.3s\n",
      "[CV 3/5] END C=0.01, penalty=none, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 2/5] END C=0.01, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 5/5] END C=0.01, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 3/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.998 total time=   7.4s\n",
      "[CV 2/5] END C=0.01, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.9s\n",
      "[CV 1/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.936 total time=   1.6s\n",
      "[CV 2/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.930 total time=   1.1s\n",
      "[CV 3/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.929 total time=   1.1s\n",
      "[CV 5/5] END ..C=0.01, penalty=l2, solver=lbfgs;, score=0.930 total time=   1.1s\n",
      "[CV 2/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.971 total time=   3.9s\n",
      "[CV 2/5] END C=0.03, penalty=none, solver=newton-cg;, score=1.000 total time=   6.8s\n",
      "[CV 1/5] END C=0.03, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 4/5] END C=0.03, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 2/5] END C=0.03, penalty=l1, solver=liblinear;, score=0.999 total time=  11.3s\n",
      "[CV 5/5] END C=0.03, penalty=l1, solver=liblinear;, score=0.999 total time=  14.8s\n",
      "[CV 2/5] END ..C=0.03, penalty=l2, solver=lbfgs;, score=0.982 total time=   1.2s\n",
      "[CV 5/5] END ..C=0.03, penalty=l2, solver=lbfgs;, score=0.990 total time=   1.2s\n",
      "[CV 3/5] END C=0.03, penalty=l2, solver=liblinear;, score=0.983 total time=   3.5s\n",
      "[CV 2/5] END C=0.05, penalty=none, solver=newton-cg;, score=1.000 total time=   6.8s\n",
      "[CV 1/5] END C=0.05, penalty=none, solver=lbfgs;, score=0.975 total time=   1.1s\n",
      "[CV 3/5] END C=0.05, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.05, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.05, penalty=l1, solver=liblinear;, score=1.000 total time=  16.2s\n",
      "[CV 5/5] END C=0.05, penalty=l1, solver=liblinear;, score=1.000 total time=  16.7s\n",
      "[CV 3/5] END C=0.05, penalty=l2, solver=liblinear;, score=0.990 total time=   2.8s\n",
      "[CV 1/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.05, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.05, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.05, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=none, solver=newton-cg;, score=0.998 total time=   7.1s\n",
      "[CV 5/5] END C=0.1, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 4/5] END C=0.1, penalty=l1, solver=liblinear;, score=1.000 total time=  15.9s\n",
      "[CV 2/5] END C=0.1, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 5/5] END C=0.1, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 1/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.965 total time=   4.0s\n",
      "[CV 5/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.972 total time=   4.0s\n",
      "[CV 4/5] END C=0.3, penalty=none, solver=newton-cg;, score=0.995 total time=   6.2s\n",
      "[CV 5/5] END .C=0.3, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV 2/5] END C=0.01, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.01, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.01, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.01, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.998 total time=   7.2s\n",
      "[CV 5/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.998 total time=   6.1s\n",
      "[CV 4/5] END C=0.01, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.8s\n",
      "[CV 3/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.961 total time=   4.1s\n",
      "[CV 3/5] END C=0.03, penalty=none, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 2/5] END C=0.03, penalty=none, solver=lbfgs;, score=0.997 total time=   1.2s\n",
      "[CV 5/5] END C=0.03, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 3/5] END C=0.03, penalty=l1, solver=liblinear;, score=1.000 total time=  15.1s\n",
      "[CV 2/5] END C=0.03, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.7s\n",
      "[CV 5/5] END C=0.03, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 4/5] END C=0.03, penalty=l2, solver=liblinear;, score=0.968 total time=   2.1s\n",
      "[CV 1/5] END C=0.03, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.03, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.03, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.03, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.03, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.03, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.03, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.03, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.03, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.03, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.03, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.03, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.03, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.03, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.03, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.05, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 5/5] END C=0.05, penalty=none, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 4/5] END C=0.05, penalty=l1, solver=liblinear;, score=0.999 total time=  16.2s\n",
      "[CV 2/5] END C=0.05, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.9s\n",
      "[CV 5/5] END C=0.05, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.2s\n",
      "[CV 1/5] END C=0.05, penalty=l2, solver=liblinear;, score=0.965 total time=   4.2s\n",
      "[CV 2/5] END C=0.1, penalty=none, solver=newton-cg;, score=1.000 total time=   7.2s\n",
      "[CV 2/5] END .C=0.1, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 4/5] END .C=0.1, penalty=none, solver=lbfgs;, score=0.991 total time=   1.1s\n",
      "[CV 4/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=l1, solver=liblinear;, score=1.000 total time=  14.8s\n",
      "[CV 5/5] END C=0.1, penalty=l1, solver=liblinear;, score=1.000 total time=  24.5s\n",
      "[CV 3/5] END C=0.3, penalty=none, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 2/5] END .C=0.3, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 4/5] END .C=0.3, penalty=none, solver=lbfgs;, score=0.991 total time=   1.1s\n",
      "[CV 2/5] END C=0.3, penalty=l1, solver=liblinear;, score=1.000 total time=  18.0s\n",
      "[CV 1/5] END C=0.3, penalty=l2, solver=newton-cg;, score=0.997 total time=   6.8s\n",
      "[CV 2/5] END C=0.3, penalty=l2, solver=newton-cg;, score=0.999 total time=   7.7s\n",
      "[CV 3/5] END C=0.3, penalty=l2, solver=newton-cg;, score=0.998 total time=   7.0s\n",
      "[CV 1/5] END ...C=0.3, penalty=l2, solver=lbfgs;, score=0.996 total time=   1.2s\n",
      "[CV 2/5] END ...C=0.3, penalty=l2, solver=lbfgs;, score=0.932 total time=   1.2s\n",
      "[CV 3/5] END ...C=0.3, penalty=l2, solver=lbfgs;, score=0.996 total time=   1.2s\n",
      "[CV 4/5] END ...C=0.3, penalty=l2, solver=lbfgs;, score=0.987 total time=   1.2s\n",
      "[CV 1/5] END C=0.3, penalty=l2, solver=liblinear;, score=0.971 total time=   4.4s\n",
      "[CV 5/5] END C=0.3, penalty=l2, solver=liblinear;, score=0.986 total time=   2.5s\n",
      "[CV 4/5] END C=0.5, penalty=none, solver=newton-cg;, score=0.995 total time=   6.4s\n",
      "[CV 3/5] END .C=0.5, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=0.5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=0.5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=0.5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=0.5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=0.5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .....C=0.5, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .....C=0.5, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .....C=0.5, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .....C=0.5, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .....C=0.5, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.5, penalty=l1, solver=liblinear;, score=1.000 total time=  29.1s\n",
      "[CV 5/5] END C=0.5, penalty=l1, solver=liblinear;, score=1.000 total time=  19.1s\n",
      "[CV 5/5] END ...C=0.5, penalty=l2, solver=lbfgs;, score=0.927 total time=   1.1s\n",
      "[CV 3/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.987 total time=   3.2s\n",
      "[CV 1/5] END C=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV 2/5] END C=0.005, penalty=l2, solver=liblinear;, score=0.955 total time=   2.2s\n",
      "[CV 5/5] END C=0.005, penalty=l2, solver=liblinear;, score=0.972 total time=   3.5s\n",
      "[CV 4/5] END C=0.01, penalty=none, solver=newton-cg;, score=0.995 total time=   6.4s\n",
      "[CV 4/5] END C=0.01, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 3/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.01, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.01, penalty=l1, solver=liblinear;, score=0.998 total time=   7.3s\n",
      "[CV 1/5] END C=0.01, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.7s\n",
      "[CV 5/5] END C=0.01, penalty=l2, solver=newton-cg;, score=0.999 total time=   7.0s\n",
      "[CV 4/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.966 total time=   1.8s\n",
      "[CV 5/5] END C=0.01, penalty=l2, solver=liblinear;, score=0.978 total time=   2.0s\n",
      "[CV 4/5] END C=0.03, penalty=none, solver=newton-cg;, score=0.995 total time=   6.3s\n",
      "[CV 3/5] END C=0.03, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.03, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.03, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.03, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.03, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.03, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.03, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.03, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.03, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.03, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.03, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.03, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ....C=0.03, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.03, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ....C=0.03, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.03, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.03, penalty=l1, solver=liblinear;, score=1.000 total time=  14.1s\n",
      "[CV 1/5] END C=0.03, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.4s\n",
      "[CV 4/5] END C=0.03, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 3/5] END ..C=0.03, penalty=l2, solver=lbfgs;, score=0.979 total time=   1.2s\n",
      "[CV 1/5] END C=0.03, penalty=l2, solver=liblinear;, score=0.984 total time=   2.2s\n",
      "[CV 5/5] END C=0.03, penalty=l2, solver=liblinear;, score=0.970 total time=   4.3s\n",
      "[CV 4/5] END C=0.05, penalty=none, solver=newton-cg;, score=0.995 total time=   6.2s\n",
      "[CV 5/5] END C=0.05, penalty=none, solver=lbfgs;, score=0.929 total time=   1.1s\n",
      "[CV 3/5] END C=0.05, penalty=l1, solver=liblinear;, score=1.000 total time=  17.5s\n",
      "[CV 1/5] END C=0.05, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.7s\n",
      "[CV 4/5] END C=0.05, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 4/5] END ..C=0.05, penalty=l2, solver=lbfgs;, score=0.979 total time=   1.2s\n",
      "[CV 2/5] END C=0.05, penalty=l2, solver=liblinear;, score=0.977 total time=   4.4s\n",
      "[CV 3/5] END C=0.1, penalty=none, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 1/5] END .C=0.1, penalty=none, solver=lbfgs;, score=0.975 total time=   1.1s\n",
      "[CV 3/5] END .C=0.1, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=0.1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=l1, solver=liblinear;, score=1.000 total time=  16.8s\n",
      "[CV 1/5] END C=0.1, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.9s\n",
      "[CV 3/5] END C=0.1, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 1/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.994 total time=   1.1s\n",
      "[CV 2/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.963 total time=   1.1s\n",
      "[CV 4/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.952 total time=   1.1s\n",
      "[CV 2/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.980 total time=   3.7s\n",
      "[CV 1/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.3, penalty=none, solver=newton-cg;, score=0.998 total time=   6.3s\n",
      "[CV 5/5] END C=0.3, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 4/5] END C=0.3, penalty=l1, solver=liblinear;, score=1.000 total time=  35.9s\n",
      "[CV 5/5] END C=0.3, penalty=l2, solver=newton-cg;, score=0.999 total time=   7.1s\n",
      "[CV 3/5] END C=0.3, penalty=l2, solver=liblinear;, score=0.969 total time=   4.3s\n",
      "[CV 2/5] END C=0.5, penalty=none, solver=newton-cg;, score=1.000 total time=   6.8s\n",
      "[CV 1/5] END .C=0.5, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 4/5] END .C=0.5, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 2/5] END C=0.5, penalty=l1, solver=liblinear;, score=1.000 total time=  35.6s\n",
      "[CV 2/5] END C=0.5, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.7s\n",
      "[CV 4/5] END C=0.5, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 1/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.982 total time=   4.6s\n",
      "[CV 3/5] END C=1, penalty=none, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 1/5] END ...C=1, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 4/5] END ...C=1, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 3/5] END .C=1, penalty=l1, solver=liblinear;, score=1.000 total time=  35.1s\n",
      "[CV 3/5] END .C=1, penalty=l2, solver=newton-cg;, score=1.000 total time=   6.9s\n",
      "[CV 1/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 2/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.997 total time=   1.2s\n",
      "[CV 3/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.991 total time=   1.4s\n",
      "[CV 5/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.925 total time=   1.3s\n",
      "[CV 2/5] END .C=1, penalty=l2, solver=liblinear;, score=0.979 total time=   4.4s\n",
      "[CV 5/5] END .C=1, penalty=l2, solver=liblinear;, score=0.991 total time=   2.2s\n",
      "[CV 3/5] END C=2, penalty=none, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 2/5] END ...C=2, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 5/5] END C=0.03, penalty=none, solver=newton-cg;, score=0.998 total time=   6.8s\n",
      "[CV 4/5] END C=0.03, penalty=l1, solver=liblinear;, score=0.999 total time=  16.0s\n",
      "[CV 3/5] END C=0.03, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 1/5] END ..C=0.03, penalty=l2, solver=lbfgs;, score=0.945 total time=   1.2s\n",
      "[CV 4/5] END ..C=0.03, penalty=l2, solver=lbfgs;, score=0.960 total time=   1.2s\n",
      "[CV 2/5] END C=0.03, penalty=l2, solver=liblinear;, score=0.988 total time=   4.2s\n",
      "[CV 3/5] END C=0.05, penalty=none, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 2/5] END C=0.05, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 4/5] END C=0.05, penalty=none, solver=lbfgs;, score=0.991 total time=   1.1s\n",
      "[CV 4/5] END C=0.05, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ....C=0.05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ....C=0.05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ....C=0.05, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.05, penalty=l1, solver=liblinear;, score=1.000 total time=  22.2s\n",
      "[CV 3/5] END C=0.05, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 1/5] END ..C=0.05, penalty=l2, solver=lbfgs;, score=0.995 total time=   1.2s\n",
      "[CV 2/5] END ..C=0.05, penalty=l2, solver=lbfgs;, score=0.954 total time=   1.1s\n",
      "[CV 3/5] END ..C=0.05, penalty=l2, solver=lbfgs;, score=0.983 total time=   1.2s\n",
      "[CV 5/5] END ..C=0.05, penalty=l2, solver=lbfgs;, score=0.994 total time=   1.2s\n",
      "[CV 4/5] END C=0.05, penalty=l2, solver=liblinear;, score=0.965 total time=   1.7s\n",
      "[CV 5/5] END C=0.05, penalty=l2, solver=liblinear;, score=0.991 total time=   4.0s\n",
      "[CV 4/5] END C=0.1, penalty=none, solver=newton-cg;, score=0.995 total time=   6.4s\n",
      "[CV 5/5] END .C=0.1, penalty=none, solver=lbfgs;, score=0.929 total time=   1.1s\n",
      "[CV 3/5] END C=0.1, penalty=l1, solver=liblinear;, score=1.000 total time=  25.0s\n",
      "[CV 4/5] END C=0.1, penalty=l2, solver=newton-cg;, score=0.995 total time=   6.0s\n",
      "[CV 3/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.919 total time=   1.1s\n",
      "[CV 5/5] END ...C=0.1, penalty=l2, solver=lbfgs;, score=0.925 total time=   1.1s\n",
      "[CV 3/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.989 total time=   2.4s\n",
      "[CV 4/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.975 total time=   2.0s\n",
      "[CV 2/5] END C=0.3, penalty=none, solver=newton-cg;, score=1.000 total time=   6.6s\n",
      "[CV 1/5] END .C=0.3, penalty=none, solver=lbfgs;, score=0.975 total time=   1.1s\n",
      "[CV 3/5] END .C=0.3, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=0.3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=0.3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=0.3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=0.3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=0.3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=0.3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .....C=0.3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .....C=0.3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .....C=0.3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .....C=0.3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .....C=0.3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.3, penalty=l1, solver=liblinear;, score=1.000 total time=  17.7s\n",
      "[CV 5/5] END C=0.3, penalty=l1, solver=liblinear;, score=1.000 total time=  33.5s\n",
      "[CV 3/5] END C=0.5, penalty=none, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 2/5] END .C=0.5, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 5/5] END .C=0.5, penalty=none, solver=lbfgs;, score=0.929 total time=   1.3s\n",
      "[CV 3/5] END C=0.5, penalty=l1, solver=liblinear;, score=1.000 total time=  30.3s\n",
      "[CV 1/5] END C=0.5, penalty=l2, solver=newton-cg;, score=0.998 total time=   7.0s\n",
      "[CV 3/5] END C=0.5, penalty=l2, solver=newton-cg;, score=1.000 total time=   6.8s\n",
      "[CV 1/5] END ...C=0.5, penalty=l2, solver=lbfgs;, score=0.990 total time=   1.1s\n",
      "[CV 2/5] END ...C=0.5, penalty=l2, solver=lbfgs;, score=0.986 total time=   1.1s\n",
      "[CV 3/5] END ...C=0.5, penalty=l2, solver=lbfgs;, score=0.999 total time=   1.1s\n",
      "[CV 4/5] END ...C=0.5, penalty=l2, solver=lbfgs;, score=0.966 total time=   1.2s\n",
      "[CV 2/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.974 total time=   3.9s\n",
      "[CV 2/5] END C=1, penalty=none, solver=newton-cg;, score=1.000 total time=   6.7s\n",
      "[CV 2/5] END ...C=1, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 5/5] END ...C=1, penalty=none, solver=lbfgs;, score=0.929 total time=   1.1s\n",
      "[CV 2/5] END .C=1, penalty=l1, solver=liblinear;, score=1.000 total time=  38.7s\n",
      "[CV 4/5] END .C=1, penalty=l2, solver=newton-cg;, score=0.996 total time=   6.9s\n",
      "[CV 4/5] END .....C=1, penalty=l2, solver=lbfgs;, score=0.956 total time=   1.3s\n",
      "[CV 1/5] END .C=1, penalty=l2, solver=liblinear;, score=0.971 total time=   4.2s\n",
      "[CV 4/5] END .C=1, penalty=l2, solver=liblinear;, score=0.994 total time=   2.3s\n",
      "[CV 2/5] END C=2, penalty=none, solver=newton-cg;, score=1.000 total time=   6.9s\n",
      "[CV 1/5] END ...C=2, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 3/5] END ...C=2, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END .C=2, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=2, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=2, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=2, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=2, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=2, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .......C=2, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .......C=2, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .......C=2, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=2, penalty=l1, solver=liblinear;, score=1.000 total time=  18.6s\n",
      "[CV 5/5] END .C=2, penalty=l1, solver=liblinear;, score=1.000 total time=  40.6s\n",
      "[CV 1/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.975 total time=   1.1s\n",
      "[CV 2/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 4/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 1/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  21.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV 3/5] END C=0.3, penalty=l1, solver=liblinear;, score=1.000 total time=  36.3s\n",
      "[CV 4/5] END C=0.3, penalty=l2, solver=newton-cg;, score=0.995 total time=   6.6s\n",
      "[CV 5/5] END ...C=0.3, penalty=l2, solver=lbfgs;, score=0.989 total time=   1.1s\n",
      "[CV 2/5] END C=0.3, penalty=l2, solver=liblinear;, score=0.994 total time=   2.5s\n",
      "[CV 4/5] END C=0.3, penalty=l2, solver=liblinear;, score=0.986 total time=   1.8s\n",
      "[CV 1/5] END C=0.3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=0.3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=0.3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=0.3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=0.3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=0.5, penalty=none, solver=newton-cg;, score=0.998 total time=   6.3s\n",
      "[CV 5/5] END C=0.5, penalty=none, solver=newton-cg;, score=0.998 total time=   6.7s\n",
      "[CV 4/5] END C=0.5, penalty=l1, solver=liblinear;, score=1.000 total time=  40.7s\n",
      "[CV 5/5] END C=0.5, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.8s\n",
      "[CV 4/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.988 total time=   1.5s\n",
      "[CV 5/5] END C=0.5, penalty=l2, solver=liblinear;, score=0.993 total time=   2.3s\n",
      "[CV 4/5] END C=1, penalty=none, solver=newton-cg;, score=0.995 total time=   6.2s\n",
      "[CV 3/5] END ...C=1, penalty=none, solver=lbfgs;, score=0.973 total time=   1.2s\n",
      "[CV 1/5] END .C=1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=1, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=1, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .......C=1, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=1, penalty=l1, solver=liblinear;, score=1.000 total time=  28.5s\n",
      "[CV 1/5] END .C=1, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 2/5] END .C=1, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 5/5] END .C=1, penalty=l2, solver=newton-cg;, score=0.998 total time=   7.3s\n",
      "[CV 3/5] END .C=1, penalty=l2, solver=liblinear;, score=0.970 total time=   4.3s\n",
      "[CV 1/5] END C=1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=1, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=2, penalty=none, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 5/5] END C=2, penalty=none, solver=newton-cg;, score=0.998 total time=   6.8s\n",
      "[CV 4/5] END .C=2, penalty=l1, solver=liblinear;, score=1.000 total time=  20.7s\n",
      "[CV 1/5] END .C=2, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 2/5] END .C=2, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 4/5] END .C=2, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.1s\n",
      "[CV 3/5] END .....C=2, penalty=l2, solver=lbfgs;, score=0.996 total time=   1.2s\n",
      "[CV 5/5] END .....C=2, penalty=l2, solver=lbfgs;, score=0.950 total time=   1.2s\n",
      "[CV 2/5] END .C=2, penalty=l2, solver=liblinear;, score=0.995 total time=   4.3s\n",
      "[CV 5/5] END .C=2, penalty=l2, solver=liblinear;, score=0.979 total time=   4.2s\n",
      "[CV 3/5] END C=3, penalty=none, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 3/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 5/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 2/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  21.7s\n",
      "[CV 2/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.1s\n",
      "[CV 5/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.2s\n",
      "[CV 2/5] END .C=3, penalty=l2, solver=liblinear;, score=0.987 total time=   2.7s\n",
      "[CV 5/5] END .C=3, penalty=l2, solver=liblinear;, score=0.986 total time=   2.1s\n",
      "[CV 4/5] END C=3, penalty=none, solver=newton-cg;, score=0.995 total time=   6.2s\n",
      "[CV 3/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=3, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=3, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  38.5s\n",
      "[CV 3/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.997 total time=   6.7s\n",
      "[CV 1/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.993 total time=   1.2s\n",
      "[CV 2/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.985 total time=   1.1s\n",
      "[CV 5/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.936 total time=   1.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV 4/5] END ...C=2, penalty=none, solver=lbfgs;, score=0.991 total time=   1.2s\n",
      "[CV 4/5] END .C=2, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=2, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=2, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=2, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .......C=2, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .......C=2, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=2, penalty=l1, solver=liblinear;, score=1.000 total time=  35.3s\n",
      "[CV 3/5] END .C=2, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 1/5] END .....C=2, penalty=l2, solver=lbfgs;, score=0.992 total time=   1.2s\n",
      "[CV 2/5] END .....C=2, penalty=l2, solver=lbfgs;, score=0.924 total time=   1.1s\n",
      "[CV 4/5] END .....C=2, penalty=l2, solver=lbfgs;, score=0.950 total time=   1.2s\n",
      "[CV 1/5] END .C=2, penalty=l2, solver=liblinear;, score=0.965 total time=   4.9s\n",
      "[CV 4/5] END .C=2, penalty=l2, solver=liblinear;, score=0.982 total time=   1.6s\n",
      "[CV 2/5] END C=3, penalty=none, solver=newton-cg;, score=1.000 total time=   6.9s\n",
      "[CV 5/5] END C=3, penalty=none, solver=newton-cg;, score=0.998 total time=   6.7s\n",
      "[CV 4/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  19.7s\n",
      "[CV 3/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.997 total time=   6.5s\n",
      "[CV 1/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.993 total time=   1.1s\n",
      "[CV 2/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.985 total time=   1.1s\n",
      "[CV 3/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.981 total time=   1.1s\n",
      "[CV 4/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.950 total time=   1.2s\n",
      "[CV 5/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.936 total time=   1.1s\n",
      "[CV 1/5] END .C=3, penalty=l2, solver=liblinear;, score=0.983 total time=   2.4s\n",
      "[CV 4/5] END .C=3, penalty=l2, solver=liblinear;, score=0.988 total time=   1.6s\n",
      "[CV 3/5] END C=3, penalty=none, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 2/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 5/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 1/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  34.3s\n",
      "[CV 1/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.3s\n",
      "[CV 4/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.2s\n",
      "[CV 3/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.981 total time=   1.1s\n",
      "[CV 1/5] END .C=3, penalty=l2, solver=liblinear;, score=0.983 total time=   2.1s\n",
      "[CV 4/5] END .C=3, penalty=l2, solver=liblinear;, score=0.988 total time=   1.5s\n",
      "[CV 1/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=4, penalty=none, solver=newton-cg;, score=1.000 total time=   6.7s\n",
      "[CV 5/5] END C=4, penalty=none, solver=newton-cg;, score=0.998 total time=   7.2s\n",
      "[CV 2/5] END .C=4, penalty=l1, solver=liblinear;, score=1.000 total time=  30.5s\n",
      "[CV 1/5] END .C=4, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.4s\n",
      "[CV 3/5] END .C=4, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 1/5] END .....C=4, penalty=l2, solver=lbfgs;, score=0.994 total time=   1.2s\n",
      "[CV 2/5] END .....C=4, penalty=l2, solver=lbfgs;, score=0.996 total time=   1.1s\n",
      "[CV 3/5] END .....C=4, penalty=l2, solver=lbfgs;, score=1.000 total time=   1.2s\n",
      "[CV 4/5] END .....C=4, penalty=l2, solver=lbfgs;, score=0.944 total time=   1.2s\n",
      "[CV 5/5] END .....C=4, penalty=l2, solver=lbfgs;, score=0.925 total time=   1.1s\n",
      "[CV 3/5] END .C=4, penalty=l2, solver=liblinear;, score=0.985 total time=   4.0s\n",
      "[CV 1/5] END C=4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=4, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=4, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=4, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=4, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=4, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=5, penalty=none, solver=newton-cg;, score=0.998 total time=   6.2s\n",
      "[CV 4/5] END C=5, penalty=none, solver=newton-cg;, score=0.995 total time=   6.4s\n",
      "[CV 1/5] END .C=5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=5, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ...C=5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ...C=5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ...C=5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ...C=5, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .......C=5, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .......C=5, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=5, penalty=l1, solver=liblinear;, score=1.000 total time=  36.8s\n",
      "[CV 4/5] END .C=5, penalty=l2, solver=newton-cg;, score=0.996 total time=   6.2s\n",
      "[CV 3/5] END .....C=5, penalty=l2, solver=lbfgs;, score=1.000 total time=   1.2s\n",
      "[CV 5/5] END .....C=5, penalty=l2, solver=lbfgs;, score=0.925 total time=   1.1s\n",
      "[CV 2/5] END .C=5, penalty=l2, solver=liblinear;, score=0.987 total time=   4.2s\n",
      "[CV 5/5] END .C=5, penalty=l2, solver=liblinear;, score=0.990 total time=   2.8s\n",
      "[CV 3/5] END C=10, penalty=none, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 1/5] END ..C=10, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 3/5] END ..C=10, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 5/5] END ..C=10, penalty=none, solver=lbfgs;, score=0.929 total time=   1.1s\n",
      "[CV 2/5] END C=10, penalty=l1, solver=liblinear;, score=1.000 total time=  21.5s\n",
      "[CV 5/5] END C=10, penalty=l1, solver=liblinear;, score=1.000 total time=  20.4s\n",
      "[CV 5/5] END C=10, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.2s\n",
      "[CV 4/5] END C=10, penalty=l2, solver=liblinear;, score=0.988 total time=   1.5s\n",
      "[CV 5/5] END C=10, penalty=l2, solver=liblinear;, score=0.994 total time=   2.7s\n",
      "[CV 4/5] END C=20, penalty=none, solver=newton-cg;, score=0.995 total time=   6.1s\n",
      "[CV 3/5] END ..C=20, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=20, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=20, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=20, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=20, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=20, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=1, penalty=none, solver=newton-cg;, score=0.998 total time=   6.3s\n",
      "[CV 5/5] END C=1, penalty=none, solver=newton-cg;, score=0.998 total time=   6.8s\n",
      "[CV 4/5] END .C=1, penalty=l1, solver=liblinear;, score=1.000 total time=  20.0s\n",
      "[CV 5/5] END .C=1, penalty=l1, solver=liblinear;, score=1.000 total time=  32.3s\n",
      "[CV 4/5] END C=2, penalty=none, solver=newton-cg;, score=0.995 total time=   6.3s\n",
      "[CV 5/5] END ...C=2, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 3/5] END .C=2, penalty=l1, solver=liblinear;, score=1.000 total time=  39.2s\n",
      "[CV 5/5] END .C=2, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 3/5] END .C=2, penalty=l2, solver=liblinear;, score=0.976 total time=   4.2s\n",
      "[CV 1/5] END C=2, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=2, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=2, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=2, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=2, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=2, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=2, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=2, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=2, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=2, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=2, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=2, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=2, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=2, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=2, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=3, penalty=none, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 4/5] END C=3, penalty=none, solver=newton-cg;, score=0.995 total time=   6.4s\n",
      "[CV 3/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  20.8s\n",
      "[CV 1/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 4/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 3/5] END .C=3, penalty=l2, solver=liblinear;, score=0.991 total time=   3.2s\n",
      "[CV 4/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=3, penalty=none, solver=newton-cg;, score=1.000 total time=   6.7s\n",
      "[CV 1/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.975 total time=   1.1s\n",
      "[CV 4/5] END ...C=3, penalty=none, solver=lbfgs;, score=0.991 total time=   1.4s\n",
      "[CV 4/5] END .......C=3, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  34.8s\n",
      "[CV 2/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.1s\n",
      "[CV 5/5] END .C=3, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 4/5] END .....C=3, penalty=l2, solver=lbfgs;, score=0.950 total time=   1.2s\n",
      "[CV 2/5] END .C=3, penalty=l2, solver=liblinear;, score=0.987 total time=   2.7s\n",
      "[CV 5/5] END .C=3, penalty=l2, solver=liblinear;, score=0.986 total time=   2.2s\n",
      "[CV 3/5] END C=4, penalty=none, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 1/5] END ...C=4, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 2/5] END ...C=4, penalty=none, solver=lbfgs;, score=0.997 total time=   1.2s\n",
      "[CV 3/5] END ...C=4, penalty=none, solver=lbfgs;, score=0.973 total time=   1.2s\n",
      "[CV 4/5] END ...C=4, penalty=none, solver=lbfgs;, score=0.991 total time=   1.4s\n",
      "[CV 5/5] END ...C=4, penalty=none, solver=lbfgs;, score=0.929 total time=   1.3s\n",
      "[CV 3/5] END .C=4, penalty=l1, solver=liblinear;, score=1.000 total time=  15.4s\n",
      "[CV 5/5] END .C=4, penalty=l1, solver=liblinear;, score=1.000 total time=  49.9s\n",
      "[CV 5/5] END ...C=5, penalty=none, solver=lbfgs;, score=0.929 total time=   1.2s\n",
      "[CV 3/5] END .C=5, penalty=l1, solver=liblinear;, score=1.000 total time=  20.5s\n",
      "[CV 1/5] END .C=5, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.9s\n",
      "[CV 2/5] END .C=5, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.6s\n",
      "[CV 3/5] END .C=5, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.1s\n",
      "[CV 5/5] END .C=5, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.7s\n",
      "[CV 3/5] END .C=5, penalty=l2, solver=liblinear;, score=0.973 total time=   4.2s\n",
      "[CV 2/5] END C=10, penalty=none, solver=newton-cg;, score=1.000 total time=   6.9s\n",
      "[CV 5/5] END C=10, penalty=none, solver=newton-cg;, score=0.998 total time=   7.1s\n",
      "[CV 4/5] END C=10, penalty=l1, solver=liblinear;, score=1.000 total time=  33.0s\n",
      "[CV 2/5] END C=10, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 1/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.989 total time=   1.1s\n",
      "[CV 2/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.979 total time=   1.1s\n",
      "[CV 4/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.986 total time=   1.1s\n",
      "[CV 1/5] END C=10, penalty=l2, solver=liblinear;, score=0.965 total time=   4.2s\n",
      "[CV 1/5] END C=10, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=10, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=10, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=10, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=10, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=20, penalty=none, solver=newton-cg;, score=1.000 total time=   6.7s\n",
      "[CV 1/5] END ..C=20, penalty=none, solver=lbfgs;, score=0.975 total time=   1.2s\n",
      "[CV 4/5] END ..C=20, penalty=none, solver=lbfgs;, score=0.991 total time=   1.3s\n",
      "[CV 3/5] END C=20, penalty=l1, solver=liblinear;, score=1.000 total time=  15.6s\n",
      "[CV 5/5] END C=20, penalty=l1, solver=liblinear;, score=1.000 total time=  20.1s\n",
      "[CV 4/5] END C=20, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.5s\n",
      "[CV 1/5] END ....C=20, penalty=l2, solver=lbfgs;, score=0.995 total time=   1.2s\n",
      "[CV 3/5] END ....C=20, penalty=l2, solver=lbfgs;, score=0.977 total time=   1.2s\n",
      "[CV 5/5] END ....C=20, penalty=l2, solver=lbfgs;, score=0.925 total time=   1.2s\n",
      "[CV 2/5] END C=20, penalty=l2, solver=liblinear;, score=0.982 total time=   4.1s\n",
      "[CV 4/5] END C=20, penalty=l2, solver=liblinear;, score=0.989 total time=   1.5s\n",
      "[CV 1/5] END C=20, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=20, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=20, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=20, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=20, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=20, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV 5/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  16.5s\n",
      "[CV 1/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=3, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=3, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=3, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=3, penalty=none, solver=newton-cg;, score=0.998 total time=   6.3s\n",
      "[CV 5/5] END C=3, penalty=none, solver=newton-cg;, score=0.998 total time=   6.6s\n",
      "[CV 4/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  18.6s\n",
      "[CV 5/5] END .C=3, penalty=l1, solver=liblinear;, score=1.000 total time=  45.0s\n",
      "[CV 4/5] END .C=4, penalty=l1, solver=liblinear;, score=1.000 total time=  41.4s\n",
      "[CV 5/5] END .C=4, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.4s\n",
      "[CV 2/5] END .C=4, penalty=l2, solver=liblinear;, score=0.994 total time=   2.7s\n",
      "[CV 4/5] END .C=4, penalty=l2, solver=liblinear;, score=0.981 total time=   1.4s\n",
      "[CV 3/5] END C=4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=4, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=4, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=4, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=4, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=4, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=4, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=5, penalty=none, solver=newton-cg;, score=1.000 total time=   6.9s\n",
      "[CV 5/5] END C=5, penalty=none, solver=newton-cg;, score=0.998 total time=   7.1s\n",
      "[CV 4/5] END .C=5, penalty=l1, solver=liblinear;, score=1.000 total time=  40.7s\n",
      "[CV 1/5] END .....C=5, penalty=l2, solver=lbfgs;, score=0.996 total time=   1.1s\n",
      "[CV 2/5] END .....C=5, penalty=l2, solver=lbfgs;, score=0.939 total time=   1.2s\n",
      "[CV 4/5] END .....C=5, penalty=l2, solver=lbfgs;, score=0.979 total time=   1.2s\n",
      "[CV 1/5] END .C=5, penalty=l2, solver=liblinear;, score=0.965 total time=   4.0s\n",
      "[CV 4/5] END .C=5, penalty=l2, solver=liblinear;, score=0.990 total time=   1.5s\n",
      "[CV 1/5] END C=5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=5, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=5, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=10, penalty=none, solver=newton-cg;, score=0.998 total time=   6.2s\n",
      "[CV 4/5] END C=10, penalty=none, solver=newton-cg;, score=0.995 total time=   6.7s\n",
      "[CV 3/5] END C=10, penalty=l1, solver=liblinear;, score=1.000 total time=  37.0s\n",
      "[CV 3/5] END C=10, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.3s\n",
      "[CV 3/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.998 total time=   1.2s\n",
      "[CV 5/5] END ....C=10, penalty=l2, solver=lbfgs;, score=0.926 total time=   1.2s\n",
      "[CV 3/5] END C=10, penalty=l2, solver=liblinear;, score=0.968 total time=   4.1s\n",
      "[CV 3/5] END C=20, penalty=none, solver=newton-cg;, score=0.999 total time=   6.2s\n",
      "[CV 2/5] END ..C=20, penalty=none, solver=lbfgs;, score=0.997 total time=   1.1s\n",
      "[CV 5/5] END ..C=20, penalty=none, solver=lbfgs;, score=0.929 total time=   1.1s\n",
      "[CV 2/5] END C=20, penalty=l1, solver=liblinear;, score=1.000 total time=  52.4s\n",
      "[CV 2/5] END C=100, penalty=none, solver=newton-cg;, score=1.000 total time=   6.5s\n",
      "[CV 1/5] END .C=100, penalty=none, solver=lbfgs;, score=0.975 total time=   1.1s\n",
      "[CV 3/5] END .C=100, penalty=none, solver=lbfgs;, score=0.973 total time=   1.1s\n",
      "[CV 1/5] END C=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .C=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .C=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .C=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .C=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .C=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .....C=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=100, penalty=l1, solver=liblinear;, score=1.000 total time=  19.8s\n",
      "[CV 5/5] END C=100, penalty=l1, solver=liblinear;, score=1.000 total time=  20.2s\n",
      "[CV 2/5] END C=100, penalty=l2, solver=newton-cg;, score=0.998 total time=   6.5s\n",
      "[CV 5/5] END C=100, penalty=l2, solver=newton-cg;, score=0.999 total time=   6.1s\n",
      "[CV 4/5] END C=100, penalty=l2, solver=liblinear;, score=0.982 total time=   1.4s\n",
      "[CV 1/5] END C=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n",
      "[CV 1/5] END C=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/5] END C=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/5] END C=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/5] END C=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/5] END C=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [1e-05, 0.0001, 0.001, 0.003, 0.005, 0.01, 0.03,\n",
       "                               0.05, 0.1, 0.3, 0.5, 1, 2, 3, 3, 4, 5, 10, 20,\n",
       "                               100],\n",
       "                         'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear']},\n",
       "             scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid Search\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "param = dict()\n",
    "param['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "param['C'] = [0.00001, 0.0001, 0.001,0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1,2,3,3,4,5,10,20, 100]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "clf1 = GridSearchCV(model,param,  scoring='accuracy', n_jobs=-1, cv=cv, verbose = 4)\n",
    "clf1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "890e37eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:43:17.003742Z",
     "iopub.status.busy": "2022-11-02T13:43:17.003345Z",
     "iopub.status.idle": "2022-11-02T13:43:17.008882Z",
     "shell.execute_reply": "2022-11-02T13:43:17.007849Z"
    },
    "papermill": {
     "duration": 0.043968,
     "end_time": "2022-11-02T13:43:17.012123",
     "exception": false,
     "start_time": "2022-11-02T13:43:16.968155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9999234711953742\n",
      "Best Hyperparameters: {'C': 5, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Score: {}'.format(clf1.best_score_))\n",
    "print('Best Hyperparameters: {}'.format(clf1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25c0fd",
   "metadata": {
    "papermill": {
     "duration": 0.033586,
     "end_time": "2022-11-02T13:43:17.080562",
     "exception": false,
     "start_time": "2022-11-02T13:43:17.046976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pisahin X, y jadi X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10506de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:43:17.150303Z",
     "iopub.status.busy": "2022-11-02T13:43:17.149557Z",
     "iopub.status.idle": "2022-11-02T13:43:17.153551Z",
     "shell.execute_reply": "2022-11-02T13:43:17.152841Z"
    },
    "papermill": {
     "duration": 0.041524,
     "end_time": "2022-11-02T13:43:17.155586",
     "exception": false,
     "start_time": "2022-11-02T13:43:17.114062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train-Test Split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78a219c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:43:17.224719Z",
     "iopub.status.busy": "2022-11-02T13:43:17.224042Z",
     "iopub.status.idle": "2022-11-02T13:43:17.243423Z",
     "shell.execute_reply": "2022-11-02T13:43:17.242149Z"
    },
    "papermill": {
     "duration": 0.056473,
     "end_time": "2022-11-02T13:43:17.245559",
     "exception": false,
     "start_time": "2022-11-02T13:43:17.189086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dataset shape:  (99960, 6)\n",
      "y_train dataset shape:  (99960,)\n",
      "X_test dataset shape:  (17641, 6)\n",
      "y_test dataset shape:  (17641,)\n"
     ]
    }
   ],
   "source": [
    "#Pisahin X, y jadi X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "# menggunakan 0.15 karena jumlah data ribuan\n",
    "print(\"X_train dataset shape: \", X_train.shape)\n",
    "print(\"y_train dataset shape: \", y_train.shape)\n",
    "print(\"X_test dataset shape: \", X_test.shape)\n",
    "print(\"y_test dataset shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b3f2a",
   "metadata": {
    "papermill": {
     "duration": 0.033499,
     "end_time": "2022-11-02T13:43:17.312930",
     "exception": false,
     "start_time": "2022-11-02T13:43:17.279431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Buat Model Pakai Best Param Dari GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d8aff18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:43:17.382788Z",
     "iopub.status.busy": "2022-11-02T13:43:17.382089Z",
     "iopub.status.idle": "2022-11-02T13:43:17.386548Z",
     "shell.execute_reply": "2022-11-02T13:43:17.385494Z"
    },
    "papermill": {
     "duration": 0.041967,
     "end_time": "2022-11-02T13:43:17.388588",
     "exception": false,
     "start_time": "2022-11-02T13:43:17.346621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Buat Model Pakai Best Param Dari GridSearchCV\n",
    "modeltun = LogisticRegression(**clf1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff99dbf",
   "metadata": {
    "papermill": {
     "duration": 0.033662,
     "end_time": "2022-11-02T13:43:17.456256",
     "exception": false,
     "start_time": "2022-11-02T13:43:17.422594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Melatih Model Pakai data train kita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41b62f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:43:17.525081Z",
     "iopub.status.busy": "2022-11-02T13:43:17.524378Z",
     "iopub.status.idle": "2022-11-02T13:43:48.722478Z",
     "shell.execute_reply": "2022-11-02T13:43:48.721595Z"
    },
    "papermill": {
     "duration": 31.235318,
     "end_time": "2022-11-02T13:43:48.724866",
     "exception": false,
     "start_time": "2022-11-02T13:43:17.489548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Melatih Model Pakai data train kita\n",
    "model_fit = modeltun.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaebe92",
   "metadata": {
    "papermill": {
     "duration": 0.033347,
     "end_time": "2022-11-02T13:43:48.791931",
     "exception": false,
     "start_time": "2022-11-02T13:43:48.758584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cek Peforma Model di Data Latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "228a7335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:43:48.860800Z",
     "iopub.status.busy": "2022-11-02T13:43:48.860222Z",
     "iopub.status.idle": "2022-11-02T13:43:49.087843Z",
     "shell.execute_reply": "2022-11-02T13:43:49.086530Z"
    },
    "papermill": {
     "duration": 0.266222,
     "end_time": "2022-11-02T13:43:49.091445",
     "exception": false,
     "start_time": "2022-11-02T13:43:48.825223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00     76170\n",
      "        True       1.00      1.00      1.00     23790\n",
      "\n",
      "    accuracy                           1.00     99960\n",
      "   macro avg       1.00      1.00      1.00     99960\n",
      "weighted avg       1.00      1.00      1.00     99960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cek Peforma model kita di data latih\n",
    "y_pred_train=model_fit.predict(X_train)\n",
    "print(classification_report(y_train,y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64753f2e",
   "metadata": {
    "papermill": {
     "duration": 0.03312,
     "end_time": "2022-11-02T13:43:49.158841",
     "exception": false,
     "start_time": "2022-11-02T13:43:49.125721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cek Peforma Model di Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "906cfc9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T13:43:49.227934Z",
     "iopub.status.busy": "2022-11-02T13:43:49.227174Z",
     "iopub.status.idle": "2022-11-02T13:43:49.296048Z",
     "shell.execute_reply": "2022-11-02T13:43:49.294492Z"
    },
    "papermill": {
     "duration": 0.109368,
     "end_time": "2022-11-02T13:43:49.301827",
     "exception": false,
     "start_time": "2022-11-02T13:43:49.192459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00     13457\n",
      "        True       1.00      1.00      1.00      4184\n",
      "\n",
      "    accuracy                           1.00     17641\n",
      "   macro avg       1.00      1.00      1.00     17641\n",
      "weighted avg       1.00      1.00      1.00     17641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cek Peforma model kita di data test\n",
    "y_pred_test=model_fit.predict(X_test)\n",
    "print(classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c41580",
   "metadata": {
    "papermill": {
     "duration": 0.033102,
     "end_time": "2022-11-02T13:43:49.400922",
     "exception": false,
     "start_time": "2022-11-02T13:43:49.367820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Acurracy 1.00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1787.163932,
   "end_time": "2022-11-02T13:43:52.059412",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-02T13:14:04.895480",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
